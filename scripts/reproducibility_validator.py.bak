#!/usr/bin/env python3
"""
Reproducibility Validator

Evaluates the reproducibility of quantum algorithms by running them multiple times and measuring result consistency. This is essential for scientific validity
of quantum-enhanced simulations in digital twin applications.
"""

import os
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import logging
import json
from scipy import stats

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dt_project.config import ConfigManager
from dt_project.quantum import initialize_quantum_components, QuantumMonteCarlo, QuantumML, ClassicalMonteCarlo

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Create output directory for plots and results
OUTPUT_DIR = "results/reproducibility"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.join(OUTPUT_DIR, "plots"), exist_ok=True)

def print_section(title):
    """Print a section title."""
    print("\n" + "=" * 80)
    print(f" {title} ".center(80, "="))
    print("=" * 80 + "\n")

def test_monte_carlo_reproducibility(n_runs=10):
    """
    Test the reproducibility of Monte Carlo simulation methods.
    
    Args:
        n_runs: Number of identical simulations to run
        
    Returns:
        Dictionary with reproducibility metrics
    """
    print_section("MONTE CARLO REPRODUCIBILITY")
    
    # Initialize quantum and classical Monte Carlo components
    config = ConfigManager()
    
    # Force quantum features to be enabled
    if 'quantum' not in config.config:
        config.config['quantum'] = {}
    config.config['quantum']['enabled'] = True
    print("Quantum features forced to enabled through config override")
    
    # Force quantum features to be enabled
    if 'quantum' not in config.config:
        config.config['quantum'] = {}
    config.config['quantum']['enabled'] = True
    print("Quantum features forced to enabled through config override")
    
    # Initialize quantum components
    quantum_components = initialize_quantum_components(config)
    quantum_mc = quantum_components["monte_carlo"]
    classical_mc = quantum_components["classical_monte_carlo"]
    
    # Parameters for the Monte Carlo simulations
    iterations = 1000
    distribution = "normal"
    
    # Define a consistent target function for all runs
    def target_function(x, y):
        """A simple 2D function for integration."""
        return np.sin(x) * np.cos(y)
    
    # Store results from each run
    quantum_results = []
    classical_results = []
    
    print(f"Running {n_runs} identical Monte Carlo simulations...")
    
    for i in range(n_runs):
        print(f"  Run {i+1}/{n_runs}")
        
        # Set random seed for reproducibility within each run
        # But allow variations between runs by using different seeds
        np.random.seed(42 + i)
        
        # Quantum Monte Carlo - using run_quantum_monte_carlo instead of integrate_2d
        try:
            quantum_start = time.time()
            param_ranges = {
                'x': (-np.pi, np.pi),
                'y': (-np.pi, np.pi)
            }
            
            quantum_result = quantum_mc.run_quantum_monte_carlo(
                param_ranges=param_ranges,
                target_function=target_function,
                iterations=iterations,
                distribution_type=distribution
            )
            quantum_time = time.time() - quantum_start
            
            quantum_results.append({
                'mean': quantum_result.get('mean', np.nan),
                'std_error': quantum_result.get('std', np.nan),
                'execution_time': quantum_time
            })
            
            print(f"    Quantum result: {quantum_result.get('mean', np.nan):.6f} ± {quantum_result.get('std', np.nan):.6f}")
        except Exception as e:
            logger.error(f"Error in quantum Monte Carlo run {i+1}: {str(e)}")
            # Add placeholder with NaN values to maintain run count
            quantum_results.append({
                'mean': np.nan,
                'std_error': np.nan,
                'execution_time': np.nan
            })
            print(f"    Quantum run failed: {str(e)}")
        
        # Classical Monte Carlo
        try:
            classical_start = time.time()
            classical_result = classical_mc.integrate_2d(
                target_function,
                x_min=-np.pi,
                x_max=np.pi,
                y_min=-np.pi,
                y_max=np.pi,
                iterations=iterations,
                distribution=distribution
            )
            classical_time = time.time() - classical_start
            
            classical_results.append({
                'mean': classical_result['mean'],
                'std_error': classical_result['std_error'],
                'execution_time': classical_time
            })
            
            print(f"    Classical result: {classical_result['mean']:.6f} ± {classical_result['std_error']:.6f}")
        except Exception as e:
            logger.error(f"Error in classical Monte Carlo run {i+1}: {str(e)}")
            # Add placeholder with NaN values to maintain run count
            classical_results.append({
                'mean': np.nan,
                'std_error': np.nan,
                'execution_time': np.nan
            })
            print(f"    Classical run failed: {str(e)}")
    
    # Filter out failed runs (with NaN values) before calculating metrics
    valid_quantum_results = [r for r in quantum_results if not np.isnan(r['mean'])]
    valid_classical_results = [r for r in classical_results if not np.isnan(r['mean'])]
    
    # Calculate reproducibility metrics
    if valid_quantum_results and valid_classical_results:
        metrics = calculate_reproducibility_metrics(valid_quantum_results, valid_classical_results)
    else:
        logger.warning("Not enough valid results to calculate reproducibility metrics")
        # Create dummy metrics with NaN values
        metrics = {
            'quantum_mean_of_means': np.nan,
            'classical_mean_of_means': np.nan,
            'quantum_std_of_means': np.nan,
            'classical_std_of_means': np.nan,
            'quantum_cv_mean': np.nan,
            'classical_cv_mean': np.nan,
            'reproducibility_ratio': np.nan,
            'quantum_max_deviation': np.nan,
            'classical_max_deviation': np.nan
        }
    
    # Generate plots
    plot_monte_carlo_reproducibility(quantum_results, classical_results, metrics)
    
    # Save results
    results = {
        'quantum_results': quantum_results,
        'classical_results': classical_results,
        'metrics': metrics
    }
    
    with open(os.path.join(OUTPUT_DIR, 'mc_reproducibility.json'), 'w') as f:
        json.dump(results, f, indent=2, default=lambda x: float(x) if isinstance(x, (np.float32, np.float64)) else x)
    
    return results

def calculate_reproducibility_metrics(quantum_results, classical_results):
    """
    Calculate metrics that quantify reproducibility of Monte Carlo simulations.
    
    Args:
        quantum_results: Results from quantum Monte Carlo simulations
        classical_results: Results from classical Monte Carlo simulations
        
    Returns:
        Dictionary with reproducibility metrics
    """
    metrics = {}
    
    # Extract mean value for each run
    quantum_means = [r['mean'] for r in quantum_results]
    classical_means = [r['mean'] for r in classical_results]
    
    # Calculate basic statistics
    metrics['quantum_mean_of_means'] = np.mean(quantum_means)
    metrics['classical_mean_of_means'] = np.mean(classical_means)
    metrics['quantum_std_of_means'] = np.std(quantum_means)
    metrics['classical_std_of_means'] = np.std(classical_means)
    
    # Calculate coefficient of variation (standard deviation / mean)
    # Add small epsilon to avoid division by zero
    epsilon = 1e-10
    metrics['quantum_cv_mean'] = metrics['quantum_std_of_means'] / (abs(metrics['quantum_mean_of_means']) + epsilon)
    metrics['classical_cv_mean'] = metrics['classical_std_of_means'] / (abs(metrics['classical_mean_of_means']) + epsilon)
    
    # Calculate the ratio of quantum CV to classical CV (reproducibility ratio)
    # If ratio < 1, quantum is MORE reproducible than classical
    # If ratio > 1, quantum is LESS reproducible than classical
    metrics['reproducibility_ratio'] = metrics['quantum_cv_mean'] / (metrics['classical_cv_mean'] + epsilon)
    
    # Calculate the maximum deviation from the mean
    metrics['quantum_max_deviation'] = max([abs(m - metrics['quantum_mean_of_means']) for m in quantum_means])
    metrics['classical_max_deviation'] = max([abs(m - metrics['classical_mean_of_means']) for m in classical_means])
    
    return metrics

def plot_monte_carlo_reproducibility(quantum_results, classical_results, metrics):
    """
    Generate plots to visualize reproducibility.
    
    Args:
        quantum_results: List of results from quantum runs
        classical_results: List of results from classical runs
        metrics: Dictionary with reproducibility metrics
    """
    plots_dir = os.path.join(OUTPUT_DIR, 'plots')
    
    # Extract data
    q_means = [r['mean'] for r in quantum_results]
    c_means = [r['mean'] for r in classical_results]
    
    # Check if we have valid data for plotting
    valid_q_means = [m for m in q_means if not np.isnan(m)]
    valid_c_means = [m for m in c_means if not np.isnan(m)]
    
    # 1. Distribution of means
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    if valid_q_means:  # Only plot if we have valid data
        plt.hist(valid_q_means, bins=min(10, max(3, len(valid_q_means))), alpha=0.7, label='Quantum')
        plt.axvline(np.mean(valid_q_means), color='r', linestyle='dashed', linewidth=1, label='Mean')
    else:
        plt.text(0.5, 0.5, "No valid quantum data", 
                 horizontalalignment='center', verticalalignment='center',
                 transform=plt.gca().transAxes)
    plt.xlabel('Mean Value')
    plt.ylabel('Frequency')
    plt.title('Distribution of Quantum Results')
    if valid_q_means:
        plt.legend()
    
    plt.subplot(1, 2, 2)
    if valid_c_means:  # Only plot if we have valid data
        plt.hist(valid_c_means, bins=min(10, max(3, len(valid_c_means))), alpha=0.7, label='Classical')
        plt.axvline(np.mean(valid_c_means), color='r', linestyle='dashed', linewidth=1, label='Mean')
    else:
        plt.text(0.5, 0.5, "No valid classical data", 
                 horizontalalignment='center', verticalalignment='center',
                 transform=plt.gca().transAxes)
    plt.xlabel('Mean Value')
    plt.ylabel('Frequency')
    plt.title('Distribution of Classical Results')
    if valid_c_means:
        plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'monte_carlo_result_distribution.png'))
    
    # 2. Run-by-run comparison
    plt.figure(figsize=(10, 6))
    runs = range(1, len(q_means) + 1)
    
    # Replace NaN with None for plotting (None values are skipped in plots)
    plot_q_means = [None if np.isnan(m) else m for m in q_means]
    plot_c_means = [None if np.isnan(m) else m for m in c_means]
    
    plt.plot(runs, plot_q_means, 'o-', label='Quantum')
    plt.plot(runs, plot_c_means, 's-', label='Classical')
    
    if valid_q_means:
        plt.axhline(np.mean(valid_q_means), color='r', linestyle='dashed', alpha=0.7, label='Quantum Mean')
    if valid_c_means:
        plt.axhline(np.mean(valid_c_means), color='b', linestyle='dashed', alpha=0.7, label='Classical Mean')
    
    plt.xlabel('Run Number')
    plt.ylabel('Mean Value')
    plt.title('Run-by-Run Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(plots_dir, 'monte_carlo_run_comparison.png'))
    
    # 3. Reproducibility metrics visualization
    plt.figure(figsize=(10, 6))
    
    # Only display metrics if we have valid data
    if not np.isnan(metrics['quantum_cv_mean']) and not np.isnan(metrics['classical_cv_mean']):
        # Calculate RSD metrics (they weren't previously included in metrics dict)
        quantum_rsd = metrics['quantum_cv_mean'] * 100  # Convert CV to percentage
        classical_rsd = metrics['classical_cv_mean'] * 100  # Convert CV to percentage
        
        metric_names = ['CV of Mean (%)', 'Max Deviation']
        quantum_values = [
            quantum_rsd,  # RSD as percentage
            metrics['quantum_max_deviation']
        ]
        classical_values = [
            classical_rsd,  # RSD as percentage
            metrics['classical_max_deviation']
        ]
        
        x = np.arange(len(metric_names))
        width = 0.35
        
        plt.bar(x - width/2, quantum_values, width, label='Quantum')
        plt.bar(x + width/2, classical_values, width, label='Classical')
        
        plt.xlabel('Metric')
        plt.ylabel('Value')
        plt.title('Reproducibility Metrics Comparison')
        plt.xticks(x, metric_names)
        plt.legend()
        plt.grid(True, alpha=0.3)
    else:
        plt.text(0.5, 0.5, "Insufficient data for metrics visualization", 
                 horizontalalignment='center', verticalalignment='center',
                 transform=plt.gca().transAxes)
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'monte_carlo_metrics_comparison.png'))

# Add fallback implementations of scikit-learn functions in case they're not available
def train_test_split_fallback(X, y, test_size=0.2, random_state=None):
    """
    A simple fallback implementation of train_test_split when scikit-learn is not available.
    
    Args:
        X: Features array
        y: Target array
        test_size: Proportion of the dataset to include in the test split
        random_state: Random seed for reproducibility
        
    Returns:
        X_train, X_test, y_train, y_test
    """
    if random_state is not None:
        np.random.seed(random_state)
        
    n_samples = len(X)
    n_test = int(n_samples * test_size)
    indices = np.random.permutation(n_samples)
    test_indices = indices[:n_test]
    train_indices = indices[n_test:]
    
    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]

def mean_squared_error_fallback(y_true, y_pred):
    """
    A simple fallback implementation of mean_squared_error when scikit-learn is not available.
    
    Args:
        y_true: True target values
        y_pred: Predicted target values
        
    Returns:
        Mean squared error
    """
    return np.mean((y_true - y_pred) ** 2)

def r2_score_fallback(y_true, y_pred):
    """
    A simple fallback implementation of r2_score when scikit-learn is not available.
    
    Args:
        y_true: True target values
        y_pred: Predicted target values
        
    Returns:
        R^2 score
    """
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    ss_res = np.sum((y_true - y_pred) ** 2)
    
    if ss_tot == 0:  # Avoid division by zero
        return 0
    
    return 1 - (ss_res / ss_tot)

def test_qml_reproducibility(n_runs=5):
    """
    Test the reproducibility of quantum machine learning algorithms.
    
    Args:
        n_runs: Number of identical runs to perform
        
    Returns:
        Dictionary with reproducibility metrics
    """
    print_section("QUANTUM MACHINE LEARNING REPRODUCIBILITY")
    
    # Initialize quantum ML
    config = ConfigManager()
    qml = QuantumML(config)
    
    # Parameters for synthetic dataset
    n_samples = 100
    n_features = 4
    test_size = 0.3
    
    # Store results from each run
    training_results = []
    prediction_results = []
    
    print(f"Running {n_runs} identical QML training and prediction cycles...")
    
    # Generate consistent synthetic dataset for all runs
    np.random.seed(42)
    X = np.random.rand(n_samples, n_features)
    y = 0.3 * X[:, 0] + 0.2 * X[:, 1] * X[:, 2] + 0.5 * X[:, 3] ** 2
    y += 0.05 * np.random.randn(n_samples)  # Add small noise
    
    # Split data - use same split for all runs to ensure fair comparison
    try:
        # Try to import from scikit-learn first
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42)
    except ImportError:
        # Fall back to our own implementation
        X_train, X_test, y_train, y_test = train_test_split_fallback(
            X, y, test_size=test_size, random_state=42)
    
    for i in range(n_runs):
        print(f"  Run {i+1}/{n_runs}")
        
        try:
            # Set the same parameters for each run
            qml.feature_map = "zz"
            qml.n_layers = 2
            qml.ansatz_type = "strongly_entangling"
            qml.max_iterations = 50
            
            # Train quantum model
            start_time = time.time()
            result = qml.train_model(X_train, y_train, test_size=0.2, verbose=False)
            training_time = time.time() - start_time
            
            # Make predictions
            y_pred = qml.predict(X_test)
            
            # Calculate metrics
            try:
                from sklearn.metrics import mean_squared_error, r2_score
            except ImportError:
                # Use fallback implementations
                mean_squared_error = mean_squared_error_fallback
                r2_score = r2_score_fallback
                
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            # Store training results
            training_results.append({
                'iterations': result.get('iterations', 0),
                'final_loss': result.get('final_loss', 0),
                'training_time': training_time,
                'loss_history': result.get('loss_history', [])[:5]  # Store just first 5 for brevity
            })
            
            # Store prediction results
            prediction_results.append({
                'mse': mse,
                'r2': r2,
                'y_pred_first5': y_pred[:5].tolist() if len(y_pred) >= 5 else y_pred.tolist()  # Store just first 5 predictions for brevity
            })
            
            print(f"    MSE: {mse:.6f}, R²: {r2:.6f}")
            
        except Exception as e:
            logger.error(f"Error in QML run {i+1}: {str(e)}")
    
    # Calculate reproducibility metrics
    metrics = calculate_qml_reproducibility_metrics(training_results, prediction_results)
    
    # Generate plots
    plot_qml_reproducibility(training_results, prediction_results, metrics)
    
    # Save results
    results = {
        'training_results': training_results,
        'prediction_results': prediction_results,
        'metrics': metrics
    }
    
    with open(os.path.join(OUTPUT_DIR, 'qml_reproducibility.json'), 'w') as f:
        json.dump(results, f, indent=2, default=lambda x: float(x) if isinstance(x, np.float32) or isinstance(x, np.float64) else x)
    
    return results

def calculate_qml_reproducibility_metrics(training_results, prediction_results):
    """
    Calculate metrics that quantify reproducibility of quantum ML algorithms.
    
    Args:
        training_results: List of results from training runs
        prediction_results: List of results from prediction runs
        
    Returns:
        Dictionary with reproducibility metrics
    """
    metrics = {}
    
    # Extract metrics
    training_times = [r['training_time'] for r in training_results]
    mse_values = [r['mse'] for r in prediction_results]
    r2_values = [r['r2'] for r in prediction_results]
    
    # Add small epsilon to avoid division by zero
    epsilon = 1e-10
    
    # Calculate coefficient of variation (CV)
    metrics['cv_training_time'] = np.std(training_times) / (np.mean(training_times) + epsilon)
    metrics['cv_mse'] = np.std(mse_values) / (np.mean(mse_values) + epsilon)
    metrics['cv_r2'] = np.std(r2_values) / (np.mean(r2_values) + epsilon)
    
    # Calculate relative standard deviation (RSD) as percentage
    metrics['rsd_mse'] = np.std(mse_values) / (abs(np.mean(mse_values)) + epsilon) * 100
    metrics['rsd_r2'] = np.std(r2_values) / (abs(np.mean(r2_values)) + epsilon) * 100
    
    # Calculate confidence intervals - handle cases with few samples better
    if len(mse_values) > 1:
        try:
            metrics['mse_95ci'] = stats.t.interval(
                0.95, len(mse_values)-1, loc=np.mean(mse_values), scale=stats.sem(mse_values)
            )
        except:
            # Fallback if stats fails (e.g. due to constant values)
            mean_mse = np.mean(mse_values)
            metrics['mse_95ci'] = (mean_mse - np.std(mse_values), mean_mse + np.std(mse_values))
    else:
        # For a single value, use a dummy confidence interval (just the value itself)
        metrics['mse_95ci'] = (mse_values[0], mse_values[0]) if mse_values else (np.nan, np.nan)
    
    if len(r2_values) > 1:
        try:
            metrics['r2_95ci'] = stats.t.interval(
                0.95, len(r2_values)-1, loc=np.mean(r2_values), scale=stats.sem(r2_values)
            )
        except:
            # Fallback if stats fails
            mean_r2 = np.mean(r2_values)
            metrics['r2_95ci'] = (mean_r2 - np.std(r2_values), mean_r2 + np.std(r2_values))
    else:
        # For a single value, use a dummy confidence interval
        metrics['r2_95ci'] = (r2_values[0], r2_values[0]) if r2_values else (np.nan, np.nan)
    
    # Calculate range (max - min)
    metrics['mse_range'] = max(mse_values) - min(mse_values) if mse_values else np.nan
    metrics['r2_range'] = max(r2_values) - min(r2_values) if r2_values else np.nan
    
    return metrics

def plot_qml_reproducibility(training_results, prediction_results, metrics):
    """
    Generate plots to visualize QML reproducibility.
    
    Args:
        training_results: List of results from training runs
        prediction_results: List of results from prediction runs
        metrics: Dictionary with reproducibility metrics
    """
    plots_dir = os.path.join(OUTPUT_DIR, 'plots')
    
    # Extract data
    mse_values = [r['mse'] for r in prediction_results]
    r2_values = [r['r2'] for r in prediction_results]
    
    # Ensure we have enough data to plot
    if not mse_values or not r2_values:
        logger.warning("Not enough data to generate QML reproducibility plots")
        return
    
    # 1. Distribution of prediction metrics
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    bins = min(10, max(3, len(mse_values)))  # At least 3 bins, at most 10
    plt.hist(mse_values, bins=bins, alpha=0.7)
    plt.axvline(np.mean(mse_values), color='r', linestyle='dashed', linewidth=1, label='Mean')
    plt.xlabel('Mean Squared Error')
    plt.ylabel('Frequency')
    plt.title('Distribution of MSE Values')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    bins = min(10, max(3, len(r2_values)))  # At least 3 bins, at most 10
    plt.hist(r2_values, bins=bins, alpha=0.7)
    plt.axvline(np.mean(r2_values), color='r', linestyle='dashed', linewidth=1, label='Mean')
    plt.xlabel('R² Score')
    plt.ylabel('Frequency')
    plt.title('Distribution of R² Values')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'qml_metrics_distribution.png'))
    
    # 2. Run-by-run comparison
    plt.figure(figsize=(10, 6))
    runs = range(1, len(mse_values) + 1)
    
    plt.subplot(2, 1, 1)
    plt.plot(runs, mse_values, 'o-')
    plt.axhline(np.mean(mse_values), color='r', linestyle='dashed', alpha=0.7, label='Mean')
    
    # Only plot confidence interval if it's valid (no NaNs)
    ci_lower, ci_upper = metrics['mse_95ci']
    if not (np.isnan(ci_lower) or np.isnan(ci_upper)):
        plt.fill_between(
            runs, 
            [ci_lower] * len(runs), 
            [ci_upper] * len(runs), 
            alpha=0.2, 
            label='95% CI'
        )
    plt.ylabel('MSE')
    plt.title('MSE by Run')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 1, 2)
    plt.plot(runs, r2_values, 'o-', color='green')
    plt.axhline(np.mean(r2_values), color='r', linestyle='dashed', alpha=0.7, label='Mean')
    
    # Only plot confidence interval if it's valid (no NaNs)
    ci_lower, ci_upper = metrics['r2_95ci']
    if not (np.isnan(ci_lower) or np.isnan(ci_upper)):
        plt.fill_between(
            runs, 
            [ci_lower] * len(runs), 
            [ci_upper] * len(runs), 
            alpha=0.2, 
            color='green', 
            label='95% CI'
        )
    plt.xlabel('Run Number')
    plt.ylabel('R²')
    plt.title('R² by Run')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'qml_run_comparison.png'))
    
    # 3. Reproducibility metrics visualization
    plt.figure(figsize=(10, 6))
    
    metric_names = ['CV of MSE', 'CV of R²', 'RSD of MSE (%)', 'RSD of R² (%)']
    values = [
        metrics['cv_mse'],
        metrics['cv_r2'],
        metrics['rsd_mse'],
        metrics['rsd_r2']
    ]
    
    # Check for NaN values and replace with zeros for plotting
    values = [0 if np.isnan(v) else v for v in values]
    
    plt.bar(metric_names, values)
    plt.ylabel('Value')
    plt.title('QML Reproducibility Metrics')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'qml_reproducibility_metrics.png'))

def main():
    """Run reproducibility validation tests."""
    print_section("REPRODUCIBILITY VALIDATION")
    print(f"Results will be saved to {os.path.abspath(OUTPUT_DIR)}")
    
    start_time = time.time()
    
    # Test Monte Carlo reproducibility
    mc_results = None
    try:
        mc_results = test_monte_carlo_reproducibility(n_runs=8)
        print("\nMonte Carlo reproducibility test completed successfully.")
    except Exception as e:
        logger.error(f"Error in Monte Carlo reproducibility test: {str(e)}")
        print(f"\nMonte Carlo reproducibility test failed: {str(e)}")
        mc_results = {
            'quantum_results': [],
            'classical_results': [],
            'metrics': {}
        }
    
    # Test QML reproducibility
    qml_results = None
    try:
        qml_results = test_qml_reproducibility(n_runs=5)
        print("\nQML reproducibility test completed successfully.")
    except Exception as e:
        logger.error(f"Error in QML reproducibility test: {str(e)}")
        print(f"\nQML reproducibility test failed: {str(e)}")
        qml_results = {
            'training_results': [],
            'prediction_results': [],
            'metrics': {}
        }
    
    # Generate summary report
    total_time = time.time() - start_time
    
    with open(os.path.join(OUTPUT_DIR, 'summary.txt'), 'w') as f:
        f.write("REPRODUCIBILITY VALIDATION SUMMARY\n")
        f.write("=================================\n\n")
        f.write(f"Validation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Total execution time: {total_time:.2f} seconds\n\n")
        
        # Monte Carlo reproducibility summary
        f.write("Monte Carlo Reproducibility:\n")
        f.write("---------------------------\n")
        
        if mc_results and mc_results.get('metrics') and 'quantum_cv_mean' in mc_results['metrics']:
            f.write(f"Coefficient of Variation (CV) of mean:\n")
            f.write(f"  Quantum: {mc_results['metrics']['quantum_cv_mean']*100:.2f}%\n")
            f.write(f"  Classical: {mc_results['metrics']['classical_cv_mean']*100:.2f}%\n\n")
            
            f.write(f"Maximum deviation from mean:\n")
            f.write(f"  Quantum: {mc_results['metrics']['quantum_max_deviation']:.6f}\n")
            f.write(f"  Classical: {mc_results['metrics']['classical_max_deviation']:.6f}\n\n")
            
            reproducibility_assessment = "More" if mc_results['metrics']['reproducibility_ratio'] < 1 else "Less"
            f.write(f"Quantum is {reproducibility_assessment} reproducible than classical (ratio: {mc_results['metrics']['reproducibility_ratio']:.2f})\n\n")
        else:
            f.write("Insufficient data to calculate Monte Carlo reproducibility metrics.\n\n")
        
        # QML reproducibility summary
        f.write("Quantum Machine Learning Reproducibility:\n")
        f.write("---------------------------------------\n")
        
        if qml_results and qml_results.get('prediction_results'):
            mean_mse = np.mean([r.get('mse', np.nan) for r in qml_results['prediction_results']])
            if not np.isnan(mean_mse):
                f.write(f"Mean Squared Error (MSE):\n")
                f.write(f"  Average: {mean_mse:.6f}\n")
                
                if 'cv_mse' in qml_results.get('metrics', {}):
                    f.write(f"  Coefficient of Variation: {qml_results['metrics']['cv_mse']*100:.2f}%\n")
                
                # Handle potential NaN values in confidence intervals
                if 'mse_95ci' in qml_results.get('metrics', {}):
                    mse_ci_lower, mse_ci_upper = qml_results['metrics']['mse_95ci']
                    if not (np.isnan(mse_ci_lower) or np.isnan(mse_ci_upper)):
                        f.write(f"  95% Confidence Interval: [{mse_ci_lower:.6f}, {mse_ci_upper:.6f}]\n\n")
                    else:
                        f.write(f"  95% Confidence Interval: Not available (insufficient data)\n\n")
                else:
                    f.write(f"  95% Confidence Interval: Not available (insufficient data)\n\n")
                
                mean_r2 = np.mean([r.get('r2', np.nan) for r in qml_results['prediction_results']])
                if not np.isnan(mean_r2):
                    f.write(f"R² Score:\n")
                    f.write(f"  Average: {mean_r2:.6f}\n")
                    
                    if 'cv_r2' in qml_results.get('metrics', {}):
                        f.write(f"  Coefficient of Variation: {qml_results['metrics']['cv_r2']*100:.2f}%\n")
                    
                    # Handle potential NaN values in confidence intervals
                    if 'r2_95ci' in qml_results.get('metrics', {}):
                        r2_ci_lower, r2_ci_upper = qml_results['metrics']['r2_95ci']
                        if not (np.isnan(r2_ci_lower) or np.isnan(r2_ci_upper)):
                            f.write(f"  95% Confidence Interval: [{r2_ci_lower:.6f}, {r2_ci_upper:.6f}]\n\n")
                        else:
                            f.write(f"  95% Confidence Interval: Not available (insufficient data)\n\n")
                    else:
                        f.write(f"  95% Confidence Interval: Not available (insufficient data)\n\n")
            else:
                f.write("Insufficient valid QML results to calculate metrics.\n\n")
        else:
            f.write("QML reproducibility test did not produce any valid results.\n\n")
        
        # Overall assessment
        f.write("Overall Assessment:\n")
        f.write("------------------\n")
        
        # Monte Carlo assessment
        if mc_results and 'reproducibility_ratio' in mc_results.get('metrics', {}):
            if mc_results['metrics']['reproducibility_ratio'] < 0.8:
                f.write("- Quantum Monte Carlo shows BETTER reproducibility than classical methods\n")
            elif mc_results['metrics']['reproducibility_ratio'] < 1.2:
                f.write("- Quantum Monte Carlo shows COMPARABLE reproducibility to classical methods\n")
            else:
                f.write("- Quantum Monte Carlo shows LOWER reproducibility than classical methods\n")
        else:
            f.write("- Quantum Monte Carlo reproducibility could not be assessed\n")
        
        # QML assessment
        if qml_results and 'cv_mse' in qml_results.get('metrics', {}) and 'cv_r2' in qml_results.get('metrics', {}):
            if qml_results['metrics']['cv_mse'] < 0.1 and qml_results['metrics']['cv_r2'] < 0.1:
                f.write("- Quantum ML shows EXCELLENT reproducibility (CV < 10%)\n")
            elif qml_results['metrics']['cv_mse'] < 0.2 and qml_results['metrics']['cv_r2'] < 0.2:
                f.write("- Quantum ML shows GOOD reproducibility (CV < 20%)\n")
            else:
                f.write("- Quantum ML shows MODERATE reproducibility (CV >= 20%)\n")
        else:
            f.write("- Quantum ML reproducibility could not be assessed\n")
        
        f.write("\nDetailed results and plots are available in the results directory.\n")
    
    # Improved JSON serialization to handle all NumPy types
    def json_numpy_serializer(obj):
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj
    
    # Save Monte Carlo results if available
    if mc_results:
        with open(os.path.join(OUTPUT_DIR, 'mc_reproducibility.json'), 'w') as f:
            json.dump(mc_results, f, indent=2, default=json_numpy_serializer)
    
    # Save QML results if available
    if qml_results:
        with open(os.path.join(OUTPUT_DIR, 'qml_reproducibility.json'), 'w') as f:
            json.dump(qml_results, f, indent=2, default=json_numpy_serializer)
    
    print("\nReproducibility validation completed!")
    print(f"Total execution time: {total_time:.2f} seconds")
    print(f"Results saved to: {os.path.abspath(OUTPUT_DIR)}")

if __name__ == "__main__":
    main() 