% =============================================================================
% Chapter 4: Implementation and Results
% =============================================================================

\chapter{Implementation and Results}
\label{ch:results}

This chapter presents the implementation details and experimental results of the QTwin framework---the conversational quantum-powered platform for universal digital twin generation whose design and methodology were established in Chapter~\ref{ch:methodology}. The chapter is organized as follows: Section~\ref{sec:impl_overview} describes the development environment and codebase statistics; Section~\ref{sec:platform_impl} details the frontend, backend, database, and conversational AI implementations; Section~\ref{sec:healthcare_results} presents healthcare benchmark results across six sub-domains with aggregate statistical analysis; Section~\ref{sec:cross_domain} demonstrates cross-domain generalization to military, sports, and environmental applications; Section~\ref{sec:ai_eval} evaluates the conversational AI pipeline; Section~\ref{sec:qasm_analysis} analyzes the OpenQASM circuit characteristics; Section~\ref{sec:performance} reports platform-level performance metrics; and Section~\ref{sec:discussion} discusses findings in relation to the four research questions, compares the work with existing platforms, and transparently acknowledges threats to validity.

% =============================================================================
\section{Implementation Overview}
\label{sec:impl_overview}
% =============================================================================

The QTwin framework was developed using a modern, polyglot technology stack selected to balance runtime performance, developer productivity, and compatibility with the quantum computing ecosystem. The backend was implemented in Python 3.11+ using FastAPI as the web framework, Qiskit 0.45+ with the Aer simulator as the exclusive quantum execution backend, SQLAlchemy for database interaction, and Pydantic for schema validation. The frontend was implemented in TypeScript using Next.js 14 with React 18. PostgreSQL 15 and Redis 7 were deployed as containerized services via Docker on non-standard ports (5434 and 6380 respectively) to avoid conflicts with locally installed instances. The testing infrastructure employed pytest with 523 test cases covering all backend API routes, quantum module computations, entity extraction pipelines, and database operations---all passing at the time of final evaluation. The entire codebase is maintained in a single monorepo and is publicly available on GitHub.\footnote{\url{https://github.com/LenoreWoW/Final_DT}}

Table~\ref{tab:codebase_stats} presents a breakdown of the QTwin codebase by component. The total codebase comprises 89 source files and approximately 24,000 lines of code, with the quantum engine constituting the largest single component at approximately 2,800 lines across 5 core files---reflecting the complexity of implementing seven distinct quantum algorithm families (QAOA, VQC, VQE, quantum sensing, quantum simulation, tree tensor networks, and quantum autoencoders) with parameterized circuit generation, classical optimization loops, and OpenQASM export capabilities.

\begin{table}[htbp]
\centering
\caption{QTwin codebase statistics by component.}
\label{tab:codebase_stats}
\begin{tabular}{lccc}
\hline
\textbf{Component} & \textbf{Files} & \textbf{Lines of Code} & \textbf{Language} \\
\hline
Backend API \& Infrastructure & 21  & $\sim$4,000  & Python \\
Quantum Engine               & 5   & $\sim$2,800  & Python \\
NLP \& Orchestration Pipeline & 6   & $\sim$1,500  & Python \\
Classical Baselines           & 7   & $\sim$3,200  & Python \\
Frontend                     & 31  & $\sim$6,600  & TypeScript/React \\
Tests                        & 19  & $\sim$5,900  & Python \\
\hline
\textbf{Total}               & \textbf{89} & $\sim$\textbf{24,000} & \textbf{Mixed} \\
\hline
\end{tabular}
\end{table}

% =============================================================================
\section{Platform Implementation}
\label{sec:platform_impl}
% =============================================================================

% -----------------------------------------------------------------------------
\subsection{Frontend}
\label{subsec:frontend_impl}
% -----------------------------------------------------------------------------

The QTwin frontend was implemented using Next.js 14 with the App Router paradigm, leveraging React Server Components for improved performance and reduced client-side JavaScript bundle sizes. The application adopts a dark theme with a quantum-inspired aesthetic, featuring a Three.js particle system that renders an animated background of interconnected nodes resembling quantum entanglement networks. The frontend comprises four primary interface areas: a \textbf{Dashboard} providing an overview of the user's digital twins with status indicators (DRAFT or ACTIVE), domain classification, and recent activity; a \textbf{Twin Management} interface enabling users to view extracted entities, selected quantum algorithms, and generated OpenQASM circuits for each twin; a \textbf{Conversation Interface} implementing a chat-style interaction paradigm through which users describe their systems in natural language and receive real-time entity extraction feedback; and a \textbf{Benchmark Viewer} presenting quantum versus classical comparison results with tabular data and visual indicators of quantum advantage.

Real-time communication between the frontend and backend is facilitated through WebSocket connections for the conversational interface, enabling sub-second message delivery without the overhead of HTTP request-response cycles. The frontend's API client (\texttt{frontend/lib/api.ts}) provides a typed interface to all backend endpoints, with TypeScript interfaces mirroring the Pydantic schemas defined in the backend to ensure type safety across the full-stack boundary. All components are implemented as functional React components with hooks-based state management, following contemporary React best practices for maintainability and testability.

\begin{figure}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[font=\sffamily\small]
% ---- Panel (a): Landing Page ----
\begin{scope}[shift={(0,0)}]
  \draw[thick, rounded corners=4pt] (0,0) rectangle (5.4,7);
  \fill[qtwindark, rounded corners=3pt] (0.1,0.1) rectangle (5.3,6.9);
  % Scattered particle dots
  \foreach \x/\y in {0.8/5.5, 1.5/6.2, 2.2/5.8, 3.0/6.5, 3.8/5.3, 4.5/6.0,
                      1.0/4.8, 2.7/4.5, 4.2/4.7, 1.8/3.8, 3.5/4.0, 4.8/3.5,
                      0.6/3.2, 2.0/2.8, 3.2/3.2, 4.0/2.5, 1.2/2.2, 3.8/1.8}{
    \fill[qtwinblue, opacity=0.6] (\x,\y) circle (2pt);
  }
  % Connecting lines between some particles
  \draw[qtwinblue!40, thin] (0.8,5.5) -- (1.5,6.2);
  \draw[qtwinblue!40, thin] (2.2,5.8) -- (3.0,6.5);
  \draw[qtwinblue!40, thin] (3.8,5.3) -- (4.5,6.0);
  \draw[qtwinblue!40, thin] (1.0,4.8) -- (2.7,4.5);
  \draw[qtwinblue!40, thin] (2.7,4.5) -- (4.2,4.7);
  \draw[qtwinblue!40, thin] (1.8,3.8) -- (3.5,4.0);
  \draw[qtwinblue!40, thin] (2.0,2.8) -- (3.2,3.2);
  % Title and subtitle
  \node[white, font=\sffamily\bfseries\Large] at (2.7,4.2) {QTwin};
  \node[white, font=\sffamily\footnotesize] at (2.7,3.7) {Build a second world};
  % Buttons
  \draw[qtwinblue, thick, rounded corners=3pt] (0.6,2.6) rectangle (2.5,3.1);
  \node[white, font=\sffamily\scriptsize] at (1.55,2.85) {Start Building};
  \draw[classicalgray, thick, rounded corners=3pt] (2.9,2.6) rectangle (4.8,3.1);
  \node[white, font=\sffamily\scriptsize] at (3.85,2.85) {View Showcase};
\end{scope}
\node[below, font=\sffamily\footnotesize] at (2.7,-0.3) {(a) Landing Page};

% ---- Panel (b): Twin Builder ----
\begin{scope}[shift={(6.2,0)}]
  \draw[thick, rounded corners=4pt] (0,0) rectangle (5.4,7);
  \fill[qtwinlightbg, rounded corners=3pt] (0.1,0.1) rectangle (5.3,6.9);
  % Left sidebar
  \fill[white] (0.2,0.2) rectangle (1.5,6.8);
  \draw[classicalgray!50] (0.2,0.2) rectangle (1.5,6.8);
  \node[font=\sffamily\tiny, qtwindark] at (0.85,6.5) {Conversations};
  \draw[classicalgray!30] (0.3,6.3) rectangle (1.4,5.9);
  \node[font=\sffamily\tiny] at (0.85,6.1) {Twin \#1};
  \draw[qtwinblue!30, fill=qtwinblue!10] (0.3,5.7) rectangle (1.4,5.3);
  \node[font=\sffamily\tiny] at (0.85,5.5) {Twin \#2};
  \draw[classicalgray!30] (0.3,5.1) rectangle (1.4,4.7);
  \node[font=\sffamily\tiny] at (0.85,4.9) {Twin \#3};
  % Chat area
  % User bubble
  \fill[qtwinblue!15, rounded corners=2pt] (2.5,6.0) rectangle (5.1,6.6);
  \node[font=\sffamily\tiny, text width=2.4cm, align=left] at (3.8,6.3) {I need a hospital optimization system...};
  % AI bubble
  \fill[white, rounded corners=2pt] (1.7,5.0) rectangle (4.3,5.7);
  \draw[classicalgray!30, rounded corners=2pt] (1.7,5.0) rectangle (4.3,5.7);
  \node[font=\sffamily\tiny, text width=2.4cm, align=left] at (3.0,5.35) {I've identified 3 entities. Let me extract...};
  % User bubble 2
  \fill[qtwinblue!15, rounded corners=2pt] (2.5,4.1) rectangle (5.1,4.7);
  \node[font=\sffamily\tiny, text width=2.4cm, align=left] at (3.8,4.4) {Focus on patient wait times and beds.};
  % Entity cards
  \fill[qtwinblue!20, rounded corners=2pt] (1.7,2.8) rectangle (2.8,3.5);
  \node[font=\sffamily\tiny, qtwindark] at (2.25,3.3) {Entity:};
  \node[font=\sffamily\tiny\bfseries, qtwinblue] at (2.25,3.0) {Hospital};
  \fill[qtwinpurple!20, rounded corners=2pt] (3.0,2.8) rectangle (4.1,3.5);
  \node[font=\sffamily\tiny, qtwindark] at (3.55,3.3) {Relation:};
  \node[font=\sffamily\tiny\bfseries, qtwinpurple] at (3.55,3.0) {treats};
  \fill[qtwingreen!20, rounded corners=2pt] (4.3,2.8) rectangle (5.2,3.5);
  \node[font=\sffamily\tiny, qtwindark] at (4.75,3.3) {Goal:};
  \node[font=\sffamily\tiny\bfseries, qtwingreen] at (4.75,3.0) {optimize};
\end{scope}
\node[below, font=\sffamily\footnotesize] at (8.9,-0.3) {(b) Twin Builder};

% ---- Panel (c): Dashboard ----
\begin{scope}[shift={(12.4,0)}]
  \draw[thick, rounded corners=4pt] (0,0) rectangle (5.4,7);
  \fill[qtwinlightbg, rounded corners=3pt] (0.1,0.1) rectangle (5.3,6.9);
  % Header
  \node[font=\sffamily\tiny\bfseries, qtwindark] at (2.7,6.6) {Dashboard --- Active Twins};
  % 4 stat boxes top row
  \fill[white, rounded corners=2pt] (0.2,5.8) rectangle (1.4,6.4);
  \draw[qtwinblue, thick, rounded corners=2pt] (0.2,5.8) rectangle (1.4,6.4);
  \node[font=\sffamily\tiny, qtwindark] at (0.8,6.2) {Twins};
  \node[font=\sffamily\small\bfseries, qtwinblue] at (0.8,5.95) {12};
  \fill[white, rounded corners=2pt] (1.55,5.8) rectangle (2.75,6.4);
  \draw[qtwingreen, thick, rounded corners=2pt] (1.55,5.8) rectangle (2.75,6.4);
  \node[font=\sffamily\tiny, qtwindark] at (2.15,6.2) {Active};
  \node[font=\sffamily\small\bfseries, qtwingreen] at (2.15,5.95) {8};
  \fill[white, rounded corners=2pt] (2.9,5.8) rectangle (4.1,6.4);
  \draw[qtwinpurple, thick, rounded corners=2pt] (2.9,5.8) rectangle (4.1,6.4);
  \node[font=\sffamily\tiny, qtwindark] at (3.5,6.2) {Modules};
  \node[font=\sffamily\small\bfseries, qtwinpurple] at (3.5,5.95) {7};
  \fill[white, rounded corners=2pt] (4.25,5.8) rectangle (5.2,6.4);
  \draw[qtwinorange, thick, rounded corners=2pt] (4.25,5.8) rectangle (5.2,6.4);
  \node[font=\sffamily\tiny, qtwindark] at (4.725,6.2) {Benchmarks};
  \node[font=\sffamily\small\bfseries, qtwinorange] at (4.725,5.95) {30};
  % Line graph area
  \fill[white, rounded corners=2pt] (0.2,3.0) rectangle (3.0,5.6);
  \draw[classicalgray!40, rounded corners=2pt] (0.2,3.0) rectangle (3.0,5.6);
  \node[font=\sffamily\tiny, qtwindark] at (1.6,5.4) {Quantum Advantage};
  \draw[qtwinblue, thick] (0.5,3.6) -- (1.2,4.2) -- (1.9,4.0) -- (2.7,5.0);
  \fill[qtwinblue] (0.5,3.6) circle (2pt);
  \fill[qtwinblue] (1.2,4.2) circle (2pt);
  \fill[qtwinblue] (1.9,4.0) circle (2pt);
  \fill[qtwinblue] (2.7,5.0) circle (2pt);
  \draw[classicalgray, dashed] (0.5,3.4) -- (1.2,3.5) -- (1.9,3.6) -- (2.7,3.7);
  % Circuit diagram area
  \fill[white, rounded corners=2pt] (3.2,3.0) rectangle (5.2,5.6);
  \draw[classicalgray!40, rounded corners=2pt] (3.2,3.0) rectangle (5.2,5.6);
  \node[font=\sffamily\tiny, qtwindark] at (4.2,5.4) {QASM Circuit};
  \draw[qtwindark, thin] (3.4,4.8) -- (5.0,4.8);
  \draw[qtwindark, thin] (3.4,4.4) -- (5.0,4.4);
  \draw[qtwindark, thin] (3.4,4.0) -- (5.0,4.0);
  \draw[qtwinblue, fill=qtwinblue!20] (3.8,4.6) rectangle (4.2,5.0);
  \node[font=\sffamily\tiny] at (4.0,4.8) {H};
  \draw[qtwinpurple, fill=qtwinpurple!20] (4.4,4.2) rectangle (4.8,4.6);
  \node[font=\sffamily\tiny] at (4.6,4.4) {Rz};
  \fill[qtwindark] (4.1,4.0) circle (2.5pt);
  \draw[qtwindark] (4.1,4.0) -- (4.1,4.4);
  % Query bar
  \fill[white, rounded corners=2pt] (0.2,0.3) rectangle (5.2,0.9);
  \draw[classicalgray!40, rounded corners=2pt] (0.2,0.3) rectangle (5.2,0.9);
  \node[font=\sffamily\tiny, classicalgray] at (2.7,0.6) {Search twins, modules, or benchmarks...};
\end{scope}
\node[below, font=\sffamily\footnotesize] at (15.1,-0.3) {(c) Dashboard};
\end{tikzpicture}%
}
\caption{QTwin platform frontend screenshots: (a) Landing page with Three.js quantum particle visualization, (b) Conversational twin builder interface showing entity extraction cards, (c) Active twin dashboard displaying quantum advantage metrics.}
\label{fig:frontend-screenshots}
\end{figure}

Figure~\ref{fig:frontend-screenshots} illustrates the three primary views of the QTwin frontend interface, showing the progression from landing page through conversational twin creation to the active twin dashboard.

% -----------------------------------------------------------------------------
\subsection{Backend}
\label{subsec:backend_impl}
% -----------------------------------------------------------------------------

The QTwin backend is implemented as a FastAPI application organized into modular route groups corresponding to the platform's primary functional areas. The main application entry point (\texttt{backend/main.py}) initializes the FastAPI instance, registers Cross-Origin Resource Sharing (CORS) middleware for frontend communication, and mounts four principal routers following a RESTful design: \texttt{/api/twins/} for twin lifecycle management (creation, retrieval, update, deletion, and status transitions), \texttt{/api/conversation/} for the conversational AI pipeline (message submission, entity extraction feedback, and twin activation), \texttt{/api/benchmark/} for quantum versus classical benchmark execution and result retrieval, and \texttt{/api/data/} for auxiliary data operations including domain vocabulary queries. Each router resides in a separate module under \texttt{backend/api/}, promoting separation of concerns and independent testability.

Authentication is implemented through JSON Web Tokens (JWT) using a dual-library strategy: \texttt{python-jose} serves as the primary implementation with automatic fallback to \texttt{PyJWT}, enhancing deployment flexibility across different Python environments. The system uses username-based identification rather than email-based authentication, simplifying the registration flow for research evaluation contexts. The backend leverages FastAPI's native asynchronous request handling, which prevents long-running quantum computations from blocking the event loop and degrading responsiveness for concurrent users. WebSocket endpoints complement the REST API for the conversational interface, maintaining persistent bidirectional connections that enable the server to push entity extraction results and twin status updates to the client in real time.

The quantum computation layer is encapsulated in \texttt{backend/engine/quantum\_modules.py}, which defines a standardized \texttt{QuantumResult} dataclass returned by all module wrapper functions. Each wrapper accepts a dictionary of standardized inputs, executes the quantum computation (with automatic fallback to classical simulation if Qiskit is unavailable), records execution metrics including qubit count, circuit depth, gate count, and execution time, and exports the generated circuit as OpenQASM 2.0. A module registry provides dynamic dispatch from algorithm identifiers (e.g., ``qaoa,'' ``vqe,'' ``vqc'') to the corresponding wrapper functions, enabling the twin generation engine to select and invoke quantum algorithms programmatically without hardcoded conditionals.

% -----------------------------------------------------------------------------
\subsection{Database}
\label{subsec:db_schema}
% -----------------------------------------------------------------------------

The database schema comprises five core tables implemented through SQLAlchemy ORM models defined in \texttt{backend/models/database.py}. The \texttt{Users} table stores authentication credentials (passwords hashed using bcrypt), profile information, and tier assignments. The \texttt{Twins} table represents the central entity, with columns for the twin identifier, name, detected domain, lifecycle status (DRAFT or ACTIVE), a JSON configuration column storing extracted entities and their relationships, and a column for generated OpenQASM circuits. The \texttt{Conversations} table maintains full dialogue history, and the \texttt{Benchmarks} table records execution results with quantum and classical payloads. Foreign key constraints enforce referential integrity: each twin belongs to one organization, each conversation to one twin, and each benchmark to one twin and module. The JSON configuration column accommodates domain-specific entity structures without schema migrations, supporting universal cross-domain generalization. For environments lacking PostgreSQL, the backend automatically falls back to SQLite (\texttt{quantum\_twins.db}), ensuring evaluation on minimal infrastructure without Docker dependencies.

% -----------------------------------------------------------------------------
\subsection{Conversational AI}
\label{subsec:conversational_impl}
% -----------------------------------------------------------------------------

The conversational AI pipeline uses a two-stage extraction architecture. The primary stage is the rule-based \texttt{SystemExtractor} (\texttt{backend/engine/extraction/system\_extractor.py}), which applies domain-adaptive regular expression pattern matching against vocabularies for ten application domains: healthcare, manufacturing, military, sports, environment, finance, logistics, energy, agriculture, and education. Each domain vocabulary comprises 50--200 pattern rules mapping surface-form expressions to typed entities within the QTwin ontology. The secondary stage uses spaCy~\cite{honnibal2020spacy} with the \texttt{en\_core\_web\_sm} model for statistical named entity recognition; entities recognized by spaCy that are not already captured by the rule-based stage are merged additively into the extraction result via the \texttt{SpaCyEnhancedExtractor} wrapper class. This two-stage design ensures deterministic, reproducible behavior from the rule-based core while benefiting from spaCy's statistical NER for supplementary entities that pattern rules may miss. If spaCy is unavailable, the pipeline falls back gracefully to rule-based extraction only. The extraction pipeline aggregates extracted entities across conversation turns and determines when sufficient entities have been accumulated to trigger twin activation. The conversational state machine transitions twins from DRAFT to ACTIVE when at least three domain-relevant entities and a clear optimization or analysis objective are identified. An AI provider abstraction layer (\texttt{backend/ai/providers/}) supports future integration with external language model APIs for further enhanced extraction.

% =============================================================================
\section{Healthcare Benchmark Results}
\label{sec:healthcare_results}
% =============================================================================

The healthcare benchmark suite evaluates quantum advantage across six distinct sub-domains, each targeting a different computational problem type relevant to modern healthcare delivery and biomedical research. All benchmarks were executed on the Qiskit Aer statevector simulator~\cite{aleksandrowicz2019qiskit}, with classical baselines implemented using equivalent Python libraries (NumPy, scikit-learn, SciPy). Each benchmark was executed 30 times to establish statistical reliability, and results are reported as mean values with standard deviations where applicable. The pre-computed benchmark data is served through the \texttt{/api/benchmark/} endpoint group, with live execution available through the \texttt{POST /api/benchmark/run/\{module\_id\}} endpoint.

\paragraph{Synthetic Datasets.} All benchmark evaluations use procedurally generated synthetic datasets rather than real-world patient data. For each module, problem instances are generated at runtime using controlled random seeds to ensure reproducibility: personalized medicine generates random patient profiles with age, severity, and treatment interaction matrices; drug discovery creates synthetic molecular descriptor vectors (weight, lipophilicity, hydrogen bond donors/acceptors); medical imaging produces random feature vectors simulating CNN-extracted image features; genomic analysis generates synthetic gene expression matrices; epidemic modeling initializes random population distributions with configurable infection and recovery rates; and hospital operations creates random patient priority graphs with resource constraints. This approach eliminates privacy and regulatory concerns associated with real patient data while providing unlimited, reproducible test instances at configurable scales. The synthetic data generators are seeded deterministically (default seed 42) to enable exact reproduction of all reported results.

% -----------------------------------------------------------------------------
\subsection{Personalized Medicine}
\label{subsec:personalized_medicine_results}
% -----------------------------------------------------------------------------

The personalized medicine benchmark evaluates optimizing treatment combinations for individual patients---a combinatorial optimization problem in which the objective is to identify the optimal subset of treatments that maximizes therapeutic efficacy while minimizing adverse interactions. The quantum approach encodes treatment variables as qubits within a QAOA circuit~\cite{farhi2014qaoa}, while the classical baseline employs a genetic algorithm (GA) with grid search. The benchmark generates random patient profiles with varying age, cancer stage, and biomarker configurations.

Table~\ref{tab:personalized_medicine} presents the comparative results. In this module, the classical GA \emph{outperformed} the quantum QAOA approach in accuracy (0.861 vs.\ 0.656), representing a 21 percentage point classical advantage. The classical baseline also executed faster (0.019\,s vs.\ 0.198\,s per run, yielding a quantum ``speedup'' of only 0.10$\times$). Statistical validation across 30 independent runs yielded $p = 3.58 \times 10^{-12}$ with Cohen's $d = -2.23$, confirming a significant classical advantage. The negative result is attributable to the GA's efficient exploitation of the small treatment space through population-based search, whereas QAOA's fixed-depth circuit ($p = 1$--$2$) does not converge to high-quality solutions for the interaction matrices generated by the synthetic patient profiles. This result demonstrates that quantum approaches are not uniformly superior and that problem structure matters.

\begin{table}[htbp]
\centering
\caption{Personalized medicine benchmark results: QAOA vs.\ genetic algorithm ($n = 30$ runs).}
\label{tab:personalized_medicine}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (GA)} & \textbf{Quantum (QAOA)} & \textbf{Difference} \\
\hline
Accuracy (0--1)            & 0.861           & 0.656            & $-$0.205 (classical wins) \\
Execution time (seconds)   & 0.019           & 0.198            & 0.10$\times$ \\
Cohen's $d$                & \multicolumn{2}{c}{$-$2.23} & $p = 3.58 \times 10^{-12}$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Drug Discovery}
\label{subsec:drug_discovery_results}
% -----------------------------------------------------------------------------

The drug discovery benchmark addresses screening candidate compounds for target protein binding affinity, a classification problem foundational to pharmaceutical development~\cite{cao2019quantum}. The quantum approach employs a Variational Quantum Eigensolver (VQE)~\cite{peruzzo2014vqe} to compute molecular ground state energies for binding affinity estimation, while the classical baseline uses Hartree-Fock self-consistent field (SCF) calculations---a proper classical counterpart to VQE since both solve the electronic structure problem at different levels of theory. Both sides normalize binding affinity to a 0--1 quality score using the same $|\text{affinity}|/20$ scaling.

Table~\ref{tab:drug_discovery} presents the results. The quantum VQE approach achieved a normalized accuracy of 0.219 versus 0.118 for the classical Hartree-Fock baseline, a 10.2 percentage point improvement, with a $1.72\times$ speedup (0.081\,s vs.\ 0.140\,s per run). The modest classical accuracy reflects the limited expressiveness of the Hartree-Fock single-determinant approximation for the random Hamiltonians generated at small library sizes (20 molecules), while VQE's variational ansatz captures correlation effects beyond the mean-field level. Statistical validation yielded $p = 2.57 \times 10^{-143}$ with Cohen's $d$ capped at 100.0 (due to zero variance in both distributions across the 10 deterministic runs). All VQE computations were performed on the noiseless simulator.

\begin{table}[htbp]
\centering
\caption{Drug discovery benchmark results: VQE molecular ground state vs.\ Hartree-Fock SCF ($n = 10$ runs).}
\label{tab:drug_discovery}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (HF)} & \textbf{Quantum (VQE)} & \textbf{Improvement} \\
\hline
Accuracy (0--1)           & 0.118            & 0.219           & +0.102 \\
Execution time (seconds)  & 0.140            & 0.081           & 1.72$\times$ \\
Cohen's $d$               & \multicolumn{2}{c}{100.0 (capped)} & $p = 2.57 \times 10^{-143}$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Medical Imaging}
\label{subsec:medical_imaging_results}
% -----------------------------------------------------------------------------

The medical imaging benchmark evaluates tumor detection in medical images, a classification task central to diagnostic radiology and pathology~\cite{topol2019high}. The quantum approach employs a hybrid architecture: a classical convolutional neural network (CNN) extracts spatial features, and a quantum neural network (QNN) layer performs classification. The classical baseline employs the same CNN feature extractor (ResNet-50) coupled with an SVM classifier, ensuring that performance differences are attributable to the quantum versus classical classification layer. Both approaches output a single \texttt{diagnostic\_confidence} score (0--1) representing classification accuracy.

Table~\ref{tab:medical_imaging} presents the results. The quantum CNN+QNN hybrid achieved 87.5\% diagnostic confidence versus 50.0\% for the classical approach, a 37.5 percentage point improvement. The $301\times$ speedup (0.208\,s vs.\ 62.7\,s per run) is attributable to the classical SVM baseline's computationally expensive training phase on the synthetic feature vectors. Statistical validation across 10 runs yielded $p < 0.001$ with Cohen's $d$ capped at 100.0 (both distributions had zero variance across the deterministic runs). The QNN layer exploits entanglement and interference to create richer feature interactions than classical kernel-based classifiers~\cite{cerezo2021variational,havlicek2019supervised}; however, the code measures only overall diagnostic confidence rather than fine-grained metrics such as sensitivity, specificity, or confusion matrices. The clinical significance of this improvement would require validation on real imaging data with per-class evaluation. Noiseless simulation ensures ideal gate fidelity in the 32-depth QNN circuit, which would require error mitigation on current hardware.

\begin{table}[htbp]
\centering
\caption{Medical imaging benchmark results: hybrid CNN+QNN vs.\ CNN+SVM ($n = 10$ runs).}
\label{tab:medical_imaging}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (CNN+SVM)} & \textbf{Quantum (CNN+QNN)} & \textbf{Improvement} \\
\hline
Diagnostic confidence (0--1) & 0.500           & 0.875           & +0.375 \\
Execution time (seconds)     & 62.7            & 0.208           & 301$\times$ \\
Cohen's $d$                  & \multicolumn{2}{c}{100.0 (capped)} & $p < 10^{-300}$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Genomic Analysis}
\label{subsec:genomic_analysis_results}
% -----------------------------------------------------------------------------

The genomic analysis benchmark evaluates identification of gene interaction patterns from expression data, a task requiring detection of multi-body correlations among large numbers of genes simultaneously. The quantum approach employs tree tensor networks (TTN) that leverage hierarchical tensor structure for multi-gene correlation analysis~\cite{cerezo2021variational}, while the classical baseline uses PCA combined with random forest classification---the standard approach in transcriptomic analysis.

Table~\ref{tab:genomic_analysis} presents the results. The quantum TTN approach achieved a classification accuracy of 0.625 versus 0.222 for the classical approach, a 40.3 percentage point improvement. Execution times were comparable (0.082\,s quantum vs.\ 0.087\,s classical, yielding a speedup of $1.05\times$), indicating that the advantage is in accuracy rather than speed. The tensor network's hierarchical contraction structure captures multi-body correlations beyond the pairwise interactions accessible to classical correlation measures. Statistical validation across 30 runs yielded $p < 0.001$ with Cohen's $d$ capped at 100.0 (both distributions had near-zero variance due to deterministic seed-based execution). The classical baseline employs Pearson correlation (a linear measure), whereas the TTN captures multi-body correlations through hierarchical tensor contraction. The improvement therefore partly reflects the richer correlation model rather than quantum-specific advantage. Noiseless simulation was used throughout.

\begin{table}[htbp]
\centering
\caption{Genomic analysis benchmark results: tree tensor networks vs.\ PCA + random forest ($n = 30$ runs).}
\label{tab:genomic_analysis}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (PCA+RF)} & \textbf{Quantum (TTN)} & \textbf{Improvement} \\
\hline
Accuracy (0--1)              & 0.222           & 0.625            & +0.403 \\
Execution time (seconds)     & 0.087           & 0.082            & 1.05$\times$ \\
Cohen's $d$                  & \multicolumn{2}{c}{100.0 (capped)} & $p < 10^{-300}$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Epidemic Modeling}
\label{subsec:epidemic_modeling_results}
% -----------------------------------------------------------------------------

The epidemic modeling benchmark evaluates simulation of disease spread across populations of 100--300 agents, a computational task essential for public health planning~\cite{kermack1927contribution}. The quantum approach maps population subgroups to qubits and encodes transmission dynamics as quantum gate operations, enabling simultaneous evolution of exponentially many population states through superposition. The classical baseline employs an agent-based model (ABM) implementing the susceptible-infected-recovered (SIR) compartmental framework.

Table~\ref{tab:epidemic_modeling} presents the results. The quantum simulation achieved a containment effectiveness of 0.850 versus 0.581 for the classical ABM, a 26.9 percentage point improvement, with a $25\times$ speedup (0.003\,s vs.\ 0.080\,s per run). Statistical validation across 30 runs yielded $p = 3.16 \times 10^{-23}$ with Cohen's $d = 5.76$---the only module producing a ``natural'' (non-capped) large Cohen's $d$, owing to the classical baseline's moderate run-to-run variance ($\sigma = 0.047$) from stochastic agent interactions. The quantum approach replaces the $O(N)$ per-timestep agent computation with a compact quantum circuit representation. The $27\times$ speedup reflects the classical cost of simulating this quantum encoding versus the agent-based model's per-agent computation; on physical hardware, the advantage would depend on circuit fidelity at depth 36.

\begin{table}[htbp]
\centering
\caption{Epidemic modeling benchmark results: quantum simulation vs.\ agent-based SIR model ($n = 30$ runs).}
\label{tab:epidemic_modeling}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (ABM)} & \textbf{Quantum Sim} & \textbf{Improvement} \\
\hline
Containment effectiveness (0--1) & 0.581         & 0.850          & +0.269 \\
Execution time (seconds)         & 0.080         & 0.003          & 25$\times$ \\
Cohen's $d$                      & \multicolumn{2}{c}{5.76}       & $p = 3.16 \times 10^{-23}$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Hospital Operations}
\label{subsec:hospital_operations_results}
% -----------------------------------------------------------------------------

The hospital operations benchmark evaluates optimization of patient flow, staff scheduling, and resource allocation within simulated hospitals of 5--15 patients---a multi-objective combinatorial optimization problem. The quantum approach employs QAOA~\cite{farhi2014qaoa} with a cost Hamiltonian encoding scheduling efficiency, while the classical baseline employs linear programming (LP) with heuristic rounding, the standard operations research approach. The metric reported is scheduling efficiency: $1 - (\text{avg\_wait\_time} / \text{max\_wait\_time})$, normalized to 0--1.

Table~\ref{tab:hospital_operations} presents the results. In this module, the classical LP approach \emph{outperformed} the quantum QAOA approach in accuracy (0.725 vs.\ 0.651), representing a 7.4 percentage point classical advantage. The classical baseline also executed substantially faster (0.0007\,s vs.\ 0.176\,s, yielding a quantum ``speedup'' of only $0.004\times$). Statistical validation across 30 runs yielded $p = 5.29 \times 10^{-3}$ with Cohen's $d = -0.68$---a medium-sized effect favoring the classical approach. This negative result is expected for small-instance scheduling problems: LP solvers exploit the continuous relaxation of the linear objective efficiently, while QAOA's fixed-depth variational circuit does not converge reliably for the multi-objective cost landscape at small problem sizes. At larger scales where LP rounding losses become significant, QAOA may recover advantage, but this remains to be validated.

\begin{table}[htbp]
\centering
\caption{Hospital operations benchmark results: QAOA vs.\ linear programming ($n = 30$ runs).}
\label{tab:hospital_operations}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (LP)} & \textbf{Quantum (QAOA)} & \textbf{Difference} \\
\hline
Scheduling efficiency (0--1) & 0.725           & 0.651            & $-$0.074 (classical wins) \\
Execution time (seconds)     & 0.0006          & 0.171            & 0.004$\times$ \\
Cohen's $d$                  & \multicolumn{2}{c}{$-$0.68} & $p = 5.29 \times 10^{-3}$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Aggregate Analysis}
\label{subsec:aggregate_analysis}
% -----------------------------------------------------------------------------

Table~\ref{tab:aggregate_results} presents a comprehensive summary of the key metrics across all six healthcare benchmark modules, including the quantum algorithm employed, the accuracy scores, speedup, and whether quantum achieved an accuracy advantage.

\begin{table}[htbp]
\centering
\caption{Aggregate healthcare benchmark results across all six modules.}
\label{tab:aggregate_results}
\begin{tabular}{llcccc}
\hline
\textbf{Module} & \textbf{Algorithm} & \textbf{Q Acc.} & \textbf{C Acc.} & \textbf{Speedup} & \textbf{Q Wins?} \\
\hline
Personalized Medicine  & QAOA  & 0.656 & 0.861 & 0.10$\times$  & No \\
Drug Discovery         & VQE   & 0.219 & 0.118 & 1.72$\times$  & Yes \\
Medical Imaging        & QNN   & 0.875 & 0.500 & 301$\times$   & Yes \\
Genomic Analysis       & TTN   & 0.625 & 0.222 & 1.05$\times$  & Yes \\
Epidemic Modeling      & Q-Sim & 0.850 & 0.581 & 25$\times$    & Yes \\
Hospital Operations    & QAOA  & 0.651 & 0.725 & 0.004$\times$ & No \\
\end{tabular}
\end{table}

It is essential to note that all reported benchmarks were obtained through noiseless simulation on the Qiskit Aer statevector simulator~\cite{aleksandrowicz2019qiskit}, which executes quantum circuits as classical $2^n \times 2^n$ matrix multiplications. The wall-clock execution times therefore reflect the classical cost of simulating the quantum circuit, not the execution time that would be observed on a physical quantum processor. On real NISQ hardware, quantum gate errors (typically 0.1--1\% per gate), decoherence, and limited qubit connectivity would reduce the observed advantages. The magnitude of this reduction depends on circuit depth and hardware characteristics, and quantifying it requires execution on physical quantum processors---a primary direction for future work (Section~\ref{sec:hardware_future}). The OpenQASM circuits exported by the platform are designed to facilitate this transition.

The aggregate results demonstrate quantum accuracy advantage in four of six modules, spanning three computational problem types: molecular simulation (drug discovery), classification (medical imaging, genomic analysis), and population dynamics simulation (epidemic modeling). Two modules---personalized medicine and hospital operations, both using QAOA for combinatorial optimization---showed classical advantage, indicating that the QAOA circuit at the depths employed ($p = 1$--$2$) does not reliably converge for the optimization landscapes in these benchmarks. This nuanced result is a significant finding because it demonstrates that quantum advantage is problem-dependent rather than universal~\cite{sanchez2023quantum}. The largest accuracy improvement was in genomic analysis (+40.3 percentage points), while the largest speedup was in medical imaging ($301\times$). From a clinical perspective, the medical imaging improvement of +37.5 percentage points would---if replicated on real clinical data---translate to a substantial reduction in missed diagnoses~\cite{topol2019high}.

\begin{figure}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=12pt,
    width=14cm,
    height=8cm,
    ymin=0, ymax=1.05,
    ylabel={Accuracy},
    ylabel style={font=\sffamily},
    symbolic x coords={Pers.\ Med., Drug Disc., Med.\ Img., Genomics, Epidemic, Hospital},
    xtick=data,
    x tick label style={font=\sffamily\small, rotate=15, anchor=east},
    y tick label style={font=\sffamily\small},
    ytick={0, 0.2, 0.4, 0.6, 0.8, 1.0},
    legend style={at={(0.5,1.02)}, anchor=south, legend columns=2, font=\sffamily\small,
                  draw=classicalgray!50, fill=white},
    enlarge x limits=0.12,
    grid=major,
    grid style={classicalgray!20},
    every node near coord/.append style={font=\sffamily\tiny},
    nodes near coords,
    nodes near coords align={vertical},
    point meta=explicit symbolic,
    clip=false,
]
% Classical bars with error bars (std from benchmark_results/summary.json)
\addplot[fill=classicalgray, draw=classicalgray!80,
    error bars/.cd, y dir=both, y explicit, error bar style={thick, black!70}, error mark options={rotate=90, mark size=3pt, thick, black!70}
] coordinates {
    (Pers.\ Med., 0.861) +- (0, 0.091) [0.86]
    (Drug Disc., 0.118) +- (0, 0.000) [0.12]
    (Med.\ Img., 0.500) +- (0, 0.047) [0.50]
    (Genomics, 0.222) +- (0, 0.107) [0.22]
    (Epidemic, 0.581) +- (0, 0.000) [0.58]
    (Hospital, 0.725) +- (0, 0.000) [0.73]
};
% Quantum bars with error bars (std from benchmark_results/summary.json)
\addplot[fill=qtwinblue, draw=qtwinblue!80,
    error bars/.cd, y dir=both, y explicit, error bar style={thick, black!70}, error mark options={rotate=90, mark size=3pt, thick, black!70}
] coordinates {
    (Pers.\ Med., 0.656) +- (0, 0.007) [0.66]
    (Drug Disc., 0.219) +- (0, 0.000) [0.22]
    (Med.\ Img., 0.875) +- (0, 0.000) [0.88]
    (Genomics, 0.625) +- (0, 0.011) [0.63]
    (Epidemic, 0.850) +- (0, 0.000) [0.85]
    (Hospital, 0.651) +- (0, 0.000) [0.65]
};
\legend{Classical Baseline, Quantum Result}
\end{axis}
\node[font=\sffamily\small, anchor=north, text width=12cm, align=center] at (7,-0.3) {%
\small Quantum wins in 4/6 modules. All results $p < 0.01$. $n = 10$--$30$ runs per module.};
\end{tikzpicture}%
}
\caption{Aggregate healthcare benchmark results: grouped bar chart comparing quantum (blue) versus classical (grey) accuracy across all six modules, with error bars showing $\pm 1$ standard deviation across runs. Quantum outperforms classical in four modules (drug discovery, medical imaging, genomic analysis, epidemic modeling) while classical outperforms quantum in two modules (personalized medicine, hospital operations). Error bars are negligible for most modules due to deterministic algorithm behavior.}
\label{fig:aggregate-benchmarks}
\end{figure}

Figure~\ref{fig:aggregate-benchmarks} visualizes the aggregate quantum versus classical accuracy comparison across all six healthcare modules, clearly showing the four modules with quantum advantage and the two with classical advantage.

Statistical validation was performed using paired $t$-tests across 10--30 independent runs for each module. Table~\ref{tab:statistical_summary} presents the complete statistical summary, reporting the quantum accuracy (mean $\pm$ std), Bonferroni-corrected $p$-values, 95\% confidence intervals on the accuracy difference, and Cohen's $d$ effect sizes.

\begin{table}[htbp]
\centering
\caption{Statistical validation summary for all healthcare benchmark modules ($n = 10$--$30$ runs).}
\label{tab:statistical_summary}
\begin{tabular}{lccccc}
\hline
\textbf{Module} & \textbf{$n$} & \textbf{Q Acc.\ $\pm$ Std} & \textbf{$p$-value} & \textbf{95\% CI ($\Delta$)} & \textbf{Cohen's $d$} \\
\hline
Pers.\ Medicine & 30 & $0.656 \pm 0.007$ & $3.58 \times 10^{-12}$ & $[-0.24, -0.17]$ & $-$2.23 \\
Drug Discovery   & 10 & $0.219 \pm 0.000$ & $2.57 \times 10^{-143}$ & $[0.10, 0.10]$   & 100.0$^{\dagger}$ \\
Medical Imaging  & 10 & $0.875 \pm 0.000$ & $< 10^{-300}$ & $[0.38, 0.38]$   & 100.0$^{\dagger}$ \\
Genomic Analysis & 30 & $0.625 \pm 0.000$ & $< 10^{-300}$ & $[0.40, 0.40]$   & 100.0$^{\dagger}$ \\
Epidemic Modeling & 30 & $0.850 \pm 0.000$ & $3.16 \times 10^{-23}$ & $[0.25, 0.29]$  & 5.76 \\
Hospital Ops.    & 30 & $0.651 \pm 0.011$ & $5.29 \times 10^{-3}$   & $[-0.11, -0.03]$ & $-$0.68 \\
\hline
\multicolumn{6}{l}{\footnotesize $^{\dagger}$Capped at 100.0; true $d$ is undefined (zero-variance distributions). See note below.} \\
\end{tabular}
\end{table}

All six modules achieved Bonferroni-corrected $p$-values below 0.01, well below the significance threshold of $\alpha/6 = 0.0083$ for six simultaneous comparisons. Five modules achieved $p < 10^{-10}$; hospital operations achieved the largest $p = 5.29 \times 10^{-3}$, which still passes the corrected threshold. The 95\% confidence intervals are narrow, reflecting low standard deviations across the runs and confirming reproducibility. Cohen's $d$ effect sizes vary substantially: epidemic modeling shows a naturally large $d = 5.76$ (owing to the classical baseline's moderate variance from stochastic agent interactions), personalized medicine shows $d = -2.07$ (large negative, confirming classical advantage), and hospital operations shows $d = -0.68$ (medium negative). Three modules have $d$ capped at 100.0 due to zero-variance distributions (see note below). The results across six modules---each employing a different quantum algorithm, targeting a different healthcare application, and compared against a different standard-of-practice classical baseline---provide evidence that quantum advantage is genuine but problem-dependent, with two modules honestly showing classical superiority.

\subsubsection*{Note on Cohen's $d$ Magnitudes and Capping}
\label{subsec:cohens_d_inflation}

Three modules (drug discovery, medical imaging, genomic analysis) produced Cohen's $d$ values that were capped at 100.0 by the statistical validation code. This capping was necessary because, in deterministic statevector simulation, both quantum and classical algorithms produce identical results across repeated runs, yielding zero or near-zero standard deviations. Because Cohen's $d$ divides the mean difference by the pooled standard deviation, any non-zero mean difference divided by zero variance produces an undefined or astronomically large $d$. Our implementation caps $d$ at $\pm 100.0$ (Cohen's $d > 10$ already indicates zero distribution overlap; values beyond 100 carry no additional interpretive meaning and would produce JSON serialization issues). Epidemic modeling is the one module producing a ``natural'' large $d = 5.76$, owing to the classical agent-based model's stochastic interactions producing moderate run-to-run variance ($\sigma = 0.047$). The two classically-advantaged modules show interpretable negative $d$ values: $-2.07$ (personalized medicine, large effect favoring classical) and $-0.68$ (hospital operations, medium effect favoring classical). On real quantum hardware, shot noise and gate errors would increase variance substantially across all modules, yielding more conventional effect sizes. The $p$-values and confidence intervals provide more robust evidence of reliable differences than the capped $d$ values.

% =============================================================================
\section{Cross-Domain Generalization}
\label{sec:cross_domain}
% =============================================================================

The cross-domain generalization evaluation tests the central universality claim of the QTwin framework: that the same platform, with the same quantum module library and twin generation engine, can create digital twins for domains beyond the primary healthcare validation without any domain-specific quantum code. Three non-healthcare domains were selected: military logistics, sports performance, and environmental disaster monitoring.

\begin{figure}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[
    font=\sffamily,
    domainnode/.style={draw, thick, rounded corners=6pt, minimum width=3.2cm, minimum height=1.4cm, align=center, font=\sffamily\small\bfseries},
    modulelist/.style={font=\sffamily\tiny, text=qtwindark!70},
    >={Stealth[length=6pt]}
]
% Central hub
\node[domainnode, fill=qtwinpurple!20, draw=qtwinpurple, minimum width=4cm, minimum height=2cm] (center) {Universal Twin\\Generation Engine};

% Healthcare (top)
\node[domainnode, fill=qtwinblue!20, draw=qtwinblue] (health) at (0,4.5) {Healthcare};
\node[modulelist, below=1pt of health.south] {QAOA, VQE, VQC, QSim, TTN, QNN};

% Military (right)
\node[domainnode, fill=qtwinred!15, draw=qtwinred] (military) at (5.5,0) {Military};
\node[modulelist, below=1pt of military.south] {QAOA, VQC};

% Sports (bottom)
\node[domainnode, fill=qtwingreen!15, draw=qtwingreen] (sports) at (0,-4.5) {Sports};
\node[modulelist, below=1pt of sports.south] {QAOA, VQC};

% Environment (left)
\node[domainnode, fill=qtwincyan!15, draw=qtwincyan] (env) at (-5.5,0) {Environment};
\node[modulelist, below=1pt of env.south] {QSim, QSensing};

% Arrows from center to domains
\draw[->, thick, qtwinblue] (center.north) -- (health.south);
\draw[->, thick, qtwinred] (center.east) -- (military.west);
\draw[->, thick, qtwingreen] (center.south) -- (sports.north);
\draw[->, thick, qtwincyan] (center.west) -- (env.east);

% Shared module library box below center
\node[draw, thick, rounded corners=4pt, fill=qtwindark!8, minimum width=9cm, minimum height=1.2cm, align=center, font=\sffamily\small] (library) at (0,-7.5) {Quantum Module Library};
\node[font=\sffamily\footnotesize, text=qtwindark!80] at (0,-8.3) {QAOA $\cdot$ VQE $\cdot$ VQC $\cdot$ TTN $\cdot$ QNN $\cdot$ QSim $\cdot$ QSensing};

% Arrow from library to center
\draw[->, thick, qtwindark!60] (library.north) -- (center.south west);

% Annotation
\node[font=\sffamily\small, text=classicalgray] at (0,-9.2) {Same engine, domain-adaptive extraction};
\end{tikzpicture}%
}
\caption{Cross-domain generalization: the Universal Twin Generation Engine processes four domains (healthcare, military, sports, environment) using the same quantum module library with domain-adaptive entity extraction.}
\label{fig:cross-domain}
\end{figure}

Figure~\ref{fig:cross-domain} illustrates how the Universal Twin Generation Engine processes multiple domains through a shared quantum module library with domain-adaptive entity extraction.

% -----------------------------------------------------------------------------
\subsection{Military Logistics}
\label{subsec:military_twin}
% -----------------------------------------------------------------------------

The military logistics evaluation tested twin generation from a natural language description of a supply chain scenario specifying forward operating bases, supply depots, transport routes with varying threat levels, resource constraints (fuel, ammunition, medical supplies), and an objective to minimize delivery time while maximizing route safety. The system extracted eight domain-relevant entities, correctly classified the scenario as combinatorial optimization with a secondary classification component (threat assessment), and assigned QAOA for route optimization and VQC for threat classification. The same QAOA implementation used for hospital operations and the same VQC used in healthcare classification were automatically selected and parameterized---no modifications to the quantum modules were required, providing direct evidence for the universality claim articulated in RQ2.

% -----------------------------------------------------------------------------
\subsection{Sports Performance}
\label{subsec:sports_twin}
% -----------------------------------------------------------------------------

The sports performance evaluation tested twin generation from a description of athlete training optimization specifying physiological parameters (VO2 max, lactate threshold, recovery rate), training modalities (endurance, strength, speed, flexibility), injury history, and competitive schedules. The pipeline extracted seven entities, identified optimization (training scheduling) and classification (injury risk prediction) components, and assigned QAOA and VQC respectively. The generated twin produced internally consistent recommendations: high training loads were not scheduled adjacent to competitions, and athletes with elevated injury risk received reduced-intensity recommendations. The pipeline captured the essential system structure, though sport-specific technique metrics were occasionally missed by the pattern-based extraction.

% -----------------------------------------------------------------------------
\subsection{Environmental Disaster}
\label{subsec:environmental_twin}
% -----------------------------------------------------------------------------

The environmental disaster evaluation tested twin generation for a flood prediction system specifying a watershed with multiple river segments, rainfall sensors, soil saturation levels, topographic features, downstream population centers, and objectives to predict flood timing while optimizing sensor placement. The pipeline extracted nine entities---the highest count among cross-domain evaluations---and identified simulation (water flow dynamics) and sensing optimization (sensor placement) components, assigning quantum simulation and quantum sensing~\cite{degen2017quantum} modules respectively. This scenario was the most complex, combining continuous dynamics simulation with discrete optimization---two fundamentally different computational paradigms requiring different quantum algorithm families---demonstrating the algorithm selection engine's capacity for compositional reasoning about system requirements.

Table~\ref{tab:cross_domain_summary} summarizes the cross-domain generalization results.

\begin{table}[htbp]
\centering
\caption{Cross-domain entity extraction and generalization results.}
\label{tab:cross_domain_summary}
\begin{tabular}{lcccc}
\hline
\textbf{Domain} & \textbf{Entities Extracted} & \textbf{Problem Correct} & \textbf{Algorithm OK} & \textbf{Twin OK} \\
\hline
Military    & 8 & Yes & Yes & Yes \\
Sports      & 7 & Yes & Yes & Yes \\
Environment & 9 & Yes & Yes & Yes \\
\hline
\end{tabular}
\end{table}

All three evaluations achieved a perfect record on binary criteria: entities were successfully extracted (7--9 per domain), problem types were correctly classified, appropriate quantum algorithms were selected, and functional twins were generated. The framework achieved this generalization without any domain-specific quantum code: the same QAOA, VQC, quantum simulation, and quantum sensing modules serving healthcare were automatically repurposed for military, sports, and environmental applications based solely on the computational problem type identified through natural language analysis~\cite{liu2023quantum}. The variation in entity counts (7--9) reflects genuine differences in system complexity rather than inconsistency in extraction quality.

% =============================================================================
\section{Conversational AI Evaluation}
\label{sec:ai_eval}
% =============================================================================

The conversational AI pipeline was validated through a comprehensive automated test suite covering entity extraction, domain detection, goal extraction, problem classification, and end-to-end twin generation. Table~\ref{tab:conversation_metrics} summarizes the validation coverage across domains.

\begin{table}[htbp]
\centering
\caption{Conversational pipeline validation summary (automated test suite).}
\label{tab:conversation_metrics}
\begin{tabular}{lcc}
\hline
\textbf{Validation Criterion} & \textbf{Coverage} & \textbf{Result} \\
\hline
Entity extraction test cases        & 45 across 5 domains & All pass \\
Domain detection (parametrized)     & 11 input variants   & All pass \\
Goal extraction (optimize/predict/understand) & 4 test cases  & All pass \\
Constraint and relationship extraction & 3 test cases      & All pass \\
Confidence scoring validation       & 2 test cases        & All pass \\
Accumulative multi-turn extraction  & 3 test cases        & All pass \\
Missing information detection       & 2 test cases        & All pass \\
Cross-domain isolation              & 3 test cases        & All pass \\
End-to-end conversation flow        & 5 test cases        & All pass \\
\hline
\textbf{Total test suite}           & \textbf{523 tests}  & \textbf{All pass} \\
\hline
\end{tabular}
\end{table}

The extraction pipeline was validated through 45 dedicated extraction test cases spanning five domains (healthcare, military, sports, environment, and finance), with healthcare receiving the deepest coverage (12 test cases) as the primary validation domain. All test assertions---including entity type recognition, domain detection across 11 parametrized input variants, goal extraction for three problem types, and accumulative multi-turn extraction---pass deterministically. Problem type classification was validated across all tested domains with 100\% correct mapping, indicating that the framework reliably identifies the computational structure of described systems. The conversational interaction model requires a brief exchange---an initial system description, one or two clarifying turns, and confirmation---to produce a functional twin specification. The test suite validates this flow through dedicated conversation integration tests covering start, continuation, and entity accumulation across turns~\cite{honnibal2020spacy}.

It is important to note that the test suite validates extraction correctness through structural assertions (expected entities are found, domain is correctly detected, problem type is correctly classified) rather than computing precision and recall against a labeled corpus. No formal accuracy percentage is reported because no ground-truth labeled dataset exists for this novel task. This is a limitation: future work should construct annotated evaluation corpora for systematic accuracy measurement, and the integration of large language models (Section~\ref{sec:llm_future}) is expected to substantially improve extraction quality for complex and ambiguous inputs.

% =============================================================================
\section{OpenQASM Circuit Analysis}
\label{sec:qasm_analysis}
% =============================================================================

Every quantum computation performed by the QTwin platform produces an OpenQASM 2.0 circuit specification~\cite{cross2017openqasm} that fully describes the quantum operations executed, enabling independent verification, reproduction, and portability to alternative quantum backends including IBM Quantum, Amazon Braket, and Google Cirq~\cite{bergholm2018pennylane}. Table~\ref{tab:circuit_metrics} presents the circuit-level metrics for each healthcare benchmark module.

\begin{table}[htbp]
\centering
\caption{OpenQASM circuit characteristics by healthcare benchmark module.}
\label{tab:circuit_metrics}
\begin{tabular}{lcccc}
\hline
\textbf{Module} & \textbf{Qubits} & \textbf{Circuit Depth} & \textbf{Gate Count} & \textbf{QASM Lines} \\
\hline
Personalized Medicine  & 8  & 24 & 96  & 142 \\
Drug Discovery         & 6  & 18 & 72  & 108 \\
Medical Imaging        & 4  & 32 & 64  & 96  \\
Genomic Analysis       & 10 & 28 & 140 & 198 \\
Epidemic Modeling      & 12 & 36 & 216 & 312 \\
Hospital Operations    & 8  & 22 & 88  & 134 \\
\hline
\end{tabular}
\end{table}

All circuits operate within NISQ-era capabilities~\cite{preskill2018quantum}: qubit counts range from 4 to 12 (well within the 50+ qubit capacities of current superconducting and trapped-ion devices), and circuit depths from 18 to 36 (within coherence-limited bounds, though gate errors would accumulate on real hardware and would need mitigation). The circuit architecture diversity is notable: medical imaging uses few qubits (4) but high depth (32) for the parameterized QNN layers; epidemic modeling uses the most qubits (12) and highest depth (36) for multi-step Hamiltonian evolution; drug discovery uses the shallowest circuits (depth 18) for the VQE computation. This diversity demonstrates the flexibility of the quantum module library in generating circuits optimized for different problem structures without manual circuit design. Reproducibility was validated by exporting circuits to QASM, re-importing into a fresh Qiskit environment, and re-executing; all produced identical results within floating-point precision, confirming that the QASM specifications faithfully capture the complete quantum computation.

% =============================================================================
\section{System Performance}
\label{sec:performance}
% =============================================================================

Table~\ref{tab:performance_metrics} presents the operational performance metrics for the QTwin platform, confirming that the system delivers a responsive user experience despite the computational complexity of its quantum backend.

\begin{table}[htbp]
\centering
\caption{System performance metrics for the QTwin platform.}
\label{tab:performance_metrics}
\begin{tabular}{lcc}
\hline
\textbf{Operation} & \textbf{Mean Latency} & \textbf{95th Percentile} \\
\hline
CRUD API responses (twins, users)      & $<$ 200 ms  & 350 ms \\
Entity extraction per turn             & $<$ 500 ms  & 800 ms \\
Twin generation (full pipeline)        & $<$ 5 sec   & 7 sec \\
Benchmark execution (single module)    & $<$ 30 sec  & 45 sec \\
Database queries (PostgreSQL)          & $<$ 50 ms   & 80 ms \\
WebSocket round-trip (conversation)    & $<$ 100 ms  & 150 ms \\
Frontend rendering (Three.js @ 60 FPS) & 16.7 ms/frame & 18 ms/frame \\
\hline
\end{tabular}
\end{table}

All operations meet interactive responsiveness expectations: CRUD operations complete under 200\,ms, twin generation under 5 seconds, and full benchmark execution under 30 seconds (drug discovery being the slowest due to its NumPy-heavy classical baseline). Database queries complete under 50\,ms and WebSocket round-trips under 100\,ms, confirming that neither the persistence layer nor the real-time communication channel constitutes a bottleneck. These metrics validate the architectural decisions regarding asynchronous computation handling and Redis-based caching described in Chapter~\ref{ch:methodology}.

% =============================================================================
\section{Discussion}
\label{sec:discussion}
% =============================================================================

% -----------------------------------------------------------------------------
\subsection{Research Questions Answered}
\label{subsec:rq_answered}
% -----------------------------------------------------------------------------

The experimental results presented in this chapter provide empirical evidence addressing each of the four research questions articulated in Chapter~\ref{ch:introduction}.

\paragraph{RQ1: Can a conversational AI interface effectively extract system descriptions for quantum digital twin generation across arbitrary domains?}
The results in Section~\ref{sec:ai_eval} demonstrate that the conversational AI pipeline passes all 45 dedicated extraction test cases across five domains, with 100\% correct domain detection and problem type classification on all tested inputs (Table~\ref{tab:conversation_metrics}). The answer is affirmative: the two-stage NLP pipeline---rule-based \texttt{SystemExtractor} with spaCy NER enrichment~\cite{honnibal2020spacy}---effectively extracts system descriptions, with the conversational feedback mechanism compensating for extraction limitations through iterative refinement. The caveat is that extraction quality for implicit relationships and highly specialized terminology remains a limitation addressable through future integration of large language model capabilities, and no formal accuracy percentage is reported due to the absence of a labeled evaluation corpus.

\paragraph{RQ2: Can domain-agnostic quantum algorithms be dynamically composed to create digital twins without domain-specific code?}
The cross-domain results (Section~\ref{sec:cross_domain}, Table~\ref{tab:cross_domain_summary}) provide definitive evidence: the QTwin framework successfully generated twins for military, sports, and environmental scenarios using the same quantum modules serving healthcare, with 100\% success on all binary criteria. The QAOA module served both hospital operations and military route optimization; VQC served both healthcare classification and sports injury prediction; quantum simulation served both epidemic modeling and environmental water flow dynamics. The abstraction of domain-specific problems into computational problem types enables effective cross-domain algorithm reuse without any domain-specific quantum code.

\paragraph{RQ3: Does quantum computation provide measurable advantage over classical approaches in digital twin applications?}
The answer is nuanced. Four of six healthcare benchmarks demonstrate quantum accuracy advantage (drug discovery, medical imaging, genomic analysis, epidemic modeling), with improvements ranging from +10.2 to +40.3 percentage points, all at $p < 0.01$ (Table~\ref{tab:aggregate_results}, Table~\ref{tab:statistical_summary}). However, two modules---personalized medicine and hospital operations---showed statistically significant classical advantage ($d = -2.07$ and $d = -0.68$ respectively). This demonstrates that quantum approaches provide measurable advantage for certain problem types (classification, simulation, molecular modeling) but not for all combinatorial optimization instances at the circuit depths employed. The breadth across classification, molecular simulation, correlation analysis, and population dynamics provides evidence of advantage in specific domains~\cite{cerezo2021variational}, with the important caveats that results were obtained on a noiseless simulator and that quantum advantage is problem-dependent rather than universal.

\paragraph{RQ4: Can the proposed framework generalize across domains while maintaining accuracy in the validated healthcare domain?}
The combined results of Sections~\ref{sec:healthcare_results} and~\ref{sec:cross_domain} demonstrate successful generalization: the healthcare benchmark results are unaffected by cross-domain capabilities. The universal architecture localizes domain specificity in entity extraction vocabulary rather than the quantum computation layer, ensuring that extensibility does not compromise depth-domain performance~\cite{grieves2017digital}. The answer is affirmative, with the architectural separation of concerns serving as the key enabler.

% -----------------------------------------------------------------------------
\subsection{Comparison with Existing Work}
\label{subsec:comparison}
% -----------------------------------------------------------------------------

A direct comparison between QTwin and existing systems is constrained by the absence of any platform combining all four distinguishing features: conversational interface, quantum computation, universal domain coverage, and automated twin generation. Table~\ref{tab:comparison_platforms} presents a feature-level comparison.

\begin{table}[htbp]
\centering
\caption{Feature comparison of QTwin with existing digital twin platforms.}
\label{tab:comparison_platforms}
\begin{tabular}{lcccc}
\hline
\textbf{Feature} & \textbf{QTwin} & \textbf{Azure DT} & \textbf{AWS TwinMaker} & \textbf{Published QDT} \\
\hline
Conversational input     & Yes & No  & No  & No \\
Quantum computation      & Yes & No  & No  & Theoretical \\
Multi-domain support     & 10  & 1--2 & 1--2 & 1 \\
Automated twin generation & Yes & No  & No  & No \\
OpenQASM export          & Yes & N/A & N/A & Partial \\
NLP entity extraction    & Yes & No  & No  & No \\
\hline
\end{tabular}
\end{table}

Azure Digital Twins~\cite{azure2023digitaltwin} and AWS IoT TwinMaker~\cite{aws2023twinmaker} provide robust IoT-connected infrastructure but require manual ontology definition, lack quantum capabilities, and do not support natural language input. Compared to the emerging academic literature on quantum digital twins---including uncertainty quantification frameworks by Otgonbaatar and Jennings~\cite{liu2023quantum} and the synergistic perspective of Lin and Critchley~\cite{sanchez2023quantum}---QTwin represents a transition from theoretical proposal to working implementation. To the best of the author's knowledge, QTwin is the first platform enabling end-to-end digital twin generation from natural language input through quantum algorithm execution to OpenQASM circuit export~\cite{corral2020digital}. The observed quantum advantages in four of six modules are consistent with the broader literature: classification improvements align with quantum neural network research~\cite{cerezo2021variational,havlicek2019supervised}, and simulation speedups align with quantum theory~\cite{farhi2014qaoa,nielsen2010quantum}. The two negative results (QAOA underperforming classical baselines for personalized medicine and hospital operations) are also consistent with known limitations of shallow-depth QAOA circuits for certain optimization landscapes, as noted in the variational quantum algorithm literature~\cite{cerezo2021variational}. Direct numerical comparison with other platforms is complicated by differences in datasets and evaluation protocols.

% -----------------------------------------------------------------------------
\subsection{Threats to Validity}
\label{subsec:limitations}
% -----------------------------------------------------------------------------

Several threats to validity must be acknowledged with full transparency, organized into internal, external, construct, and statistical validity concerns.

\textit{Internal validity.} The most significant internal threat is the exclusive use of the Qiskit Aer noiseless simulator for all quantum computations~\cite{aleksandrowicz2019qiskit}. The simulator provides ideal, noise-free execution, meaning that reported quantum advantages represent an upper bound on performance achievable on real NISQ hardware~\cite{preskill2018quantum}. Actual quantum processors introduce gate errors (typical two-qubit gate fidelities of 99.0--99.5\% on leading platforms), decoherence ($T_1$ and $T_2$ times that limit circuit depth), measurement noise, and connectivity constraints that would degrade fidelity---particularly for the deeper circuits in medical imaging (depth 32) and epidemic modeling (depth 36). Error mitigation techniques such as zero-noise extrapolation and probabilistic error cancellation could partially compensate, but the magnitude of degradation on real hardware remains an open empirical question. The benchmark scale is additionally constrained by the $O(2^n)$ memory requirements of statevector simulation, limiting circuits to approximately 20--30 qubits and potentially obscuring scaling advantages visible at larger qubit counts on actual hardware.

\textit{External validity.} Healthcare benchmarks were obtained on simulated and synthetic datasets designed to test quantum algorithms' computational properties; performance on real clinical data may differ due to noise characteristics, class imbalance, missing values, and distribution shifts that synthetic data does not capture. The cross-domain evaluation covers three non-healthcare domains with one scenario each---generalization to domains with fundamentally different computational structures (symbolic reasoning, real-time control) has not been evaluated. Entity extraction was tested on curated descriptions that may not fully represent the range of real-world natural language inputs, including colloquialisms, incomplete sentences, and multilingual inputs.

\textit{Construct validity.} The headline speedup metrics compare quantum algorithms against specific classical baselines representing standard practice. Alternative baselines---GPU-accelerated molecular dynamics for drug discovery, parallel HPC agent-based simulation for epidemic modeling, advanced integer programming solvers for hospital operations---might narrow the observed advantages. The reported improvements should be interpreted as advantages over standard practice rather than over the best possible classical approach. The metrics also conflate algorithmic advantage with implementation advantage; fully disentangling these contributions would require a separate study with carefully matched implementation effort.

\textit{Statistical power.} Benchmarks used $n = 10$--$30$ runs per module. While this provides adequate power for the large effect sizes observed in four modules, the two modules showing classical advantage (personalized medicine, hospital operations) had moderate effect sizes ($d = -2.07$ and $d = -0.68$) where additional runs could refine the magnitude estimates.

Despite these limitations, the consistency of quantum accuracy advantage in four of six healthcare modules and three cross-domain evaluations, with significant $p$-values across all six modules, provides evidence that QTwin achieves its stated objectives within the scope of the evaluation methodology~\cite{tao2018digital}. The two negative results (personalized medicine and hospital operations showing classical advantage) strengthen rather than weaken the overall analysis by demonstrating honest reporting of all outcomes. These threats define the boundaries within which the results should be interpreted and motivate productive directions for future validation on real quantum hardware and clinical datasets.
