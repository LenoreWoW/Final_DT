% =============================================================================
% Chapter 4: Implementation and Results
% =============================================================================

\chapter{Implementation and Results}
\label{ch:results}

This chapter presents the implementation details and experimental results of the QTwin framework---the conversational quantum-powered platform for universal digital twin generation whose design and methodology were established in Chapter~\ref{ch:methodology}. The chapter is organized as follows: Section~\ref{sec:impl_overview} describes the development environment and codebase statistics; Section~\ref{sec:platform_impl} details the frontend, backend, database, and conversational AI implementations; Section~\ref{sec:healthcare_results} presents healthcare benchmark results across six sub-domains with aggregate statistical analysis; Section~\ref{sec:cross_domain} demonstrates cross-domain generalization to military, sports, and environmental applications; Section~\ref{sec:ai_eval} evaluates the conversational AI pipeline; Section~\ref{sec:qasm_analysis} analyzes the OpenQASM circuit characteristics; Section~\ref{sec:performance} reports platform-level performance metrics; and Section~\ref{sec:discussion} discusses findings in relation to the four research questions, compares the work with existing platforms, and transparently acknowledges threats to validity.

% =============================================================================
\section{Implementation Overview}
\label{sec:impl_overview}
% =============================================================================

The QTwin framework was developed using a modern, polyglot technology stack selected to balance runtime performance, developer productivity, and compatibility with the quantum computing ecosystem. The backend was implemented in Python 3.11+ using FastAPI as the web framework, Qiskit 0.45+ with the Aer simulator as the exclusive quantum execution backend, SQLAlchemy for database interaction, and Pydantic for schema validation. The frontend was implemented in TypeScript using Next.js 14 with React 18. PostgreSQL 15 and Redis 7 were deployed as containerized services via Docker on non-standard ports (5434 and 6380 respectively) to avoid conflicts with locally installed instances. The testing infrastructure employed pytest with 488 test cases covering all backend API routes, quantum module computations, entity extraction pipelines, and database operations---all passing at the time of final evaluation. The entire codebase is maintained in a single monorepo and is publicly available on GitHub.

Table~\ref{tab:codebase_stats} presents a breakdown of the QTwin codebase by component. The total codebase comprises over 90 files and approximately 22,000 lines of code, with quantum modules constituting the largest single component at approximately 5,000 lines across 26 files---reflecting the complexity of implementing seven distinct quantum algorithm families (QAOA, VQC, QSVM, VQE, quantum simulation, tree tensor networks, and quantum autoencoders) with parameterized circuit generation, classical optimization loops, and OpenQASM export capabilities.

\begin{table}[htbp]
\centering
\caption{QTwin codebase statistics by component.}
\label{tab:codebase_stats}
\begin{tabular}{lccc}
\hline
\textbf{Component} & \textbf{Files} & \textbf{Lines of Code} & \textbf{Language} \\
\hline
Backend API        & 15+  & $\sim$3,000  & Python \\
Quantum Modules    & 26   & $\sim$5,000  & Python \\
Healthcare Modules & 10   & $\sim$2,000  & Python \\
NLP Engine         & 5    & $\sim$1,500  & Python \\
Frontend           & 20+  & $\sim$4,000  & TypeScript/React \\
Tests              & 18   & $\sim$5,500  & Python \\
\hline
\textbf{Total}     & \textbf{90+} & $\sim$\textbf{22,000} & \textbf{Mixed} \\
\hline
\end{tabular}
\end{table}

% =============================================================================
\section{Platform Implementation}
\label{sec:platform_impl}
% =============================================================================

% -----------------------------------------------------------------------------
\subsection{Frontend}
\label{subsec:frontend_impl}
% -----------------------------------------------------------------------------

The QTwin frontend was implemented using Next.js 14 with the App Router paradigm, leveraging React Server Components for improved performance and reduced client-side JavaScript bundle sizes. The application adopts a dark theme with a quantum-inspired aesthetic, featuring a Three.js particle system that renders an animated background of interconnected nodes resembling quantum entanglement networks. The frontend comprises four primary interface areas: a \textbf{Dashboard} providing an overview of the user's digital twins with status indicators (DRAFT or ACTIVE), domain classification, and recent activity; a \textbf{Twin Management} interface enabling users to view extracted entities, selected quantum algorithms, and generated OpenQASM circuits for each twin; a \textbf{Conversation Interface} implementing a chat-style interaction paradigm through which users describe their systems in natural language and receive real-time entity extraction feedback; and a \textbf{Benchmark Viewer} presenting quantum versus classical comparison results with tabular data and visual indicators of quantum advantage.

Real-time communication between the frontend and backend is facilitated through WebSocket connections for the conversational interface, enabling sub-second message delivery without the overhead of HTTP request-response cycles. The frontend's API client (\texttt{frontend/lib/api.ts}) provides a typed interface to all backend endpoints, with TypeScript interfaces mirroring the Pydantic schemas defined in the backend to ensure type safety across the full-stack boundary. All components are implemented as functional React components with hooks-based state management, following contemporary React best practices for maintainability and testability.

\begin{figure}[htbp]
\centering
\fbox{\begin{minipage}{0.85\textwidth}
\centering
\vspace{2.5cm}
{\small\textit{[QTwin platform frontend screenshots: (a) Landing page with Three.js quantum particle visualization, (b) Conversational twin builder interface showing entity extraction cards, (c) Active twin dashboard displaying quantum advantage metrics.]}}
\vspace{2.5cm}
\end{minipage}}
\caption{QTwin platform frontend screenshots: (a) Landing page with Three.js quantum particle visualization, (b) Conversational twin builder interface showing entity extraction cards, (c) Active twin dashboard displaying quantum advantage metrics.}
\label{fig:frontend-screenshots}
\end{figure}

Figure~\ref{fig:frontend-screenshots} illustrates the three primary views of the QTwin frontend interface, showing the progression from landing page through conversational twin creation to the active twin dashboard.

% -----------------------------------------------------------------------------
\subsection{Backend}
\label{subsec:backend_impl}
% -----------------------------------------------------------------------------

The QTwin backend is implemented as a FastAPI application organized into modular route groups corresponding to the platform's primary functional areas. The main application entry point (\texttt{backend/main.py}) initializes the FastAPI instance, registers Cross-Origin Resource Sharing (CORS) middleware for frontend communication, and mounts four principal routers following a RESTful design: \texttt{/api/twins/} for twin lifecycle management (creation, retrieval, update, deletion, and status transitions), \texttt{/api/conversation/} for the conversational AI pipeline (message submission, entity extraction feedback, and twin activation), \texttt{/api/benchmark/} for quantum versus classical benchmark execution and result retrieval, and \texttt{/api/data/} for auxiliary data operations including domain vocabulary queries. Each router resides in a separate module under \texttt{backend/api/}, promoting separation of concerns and independent testability.

Authentication is implemented through JSON Web Tokens (JWT) using a dual-library strategy: \texttt{python-jose} serves as the primary implementation with automatic fallback to \texttt{PyJWT}, enhancing deployment flexibility across different Python environments. The system uses username-based identification rather than email-based authentication, simplifying the registration flow for research evaluation contexts. The backend leverages FastAPI's native asynchronous request handling, which prevents long-running quantum computations from blocking the event loop and degrading responsiveness for concurrent users. WebSocket endpoints complement the REST API for the conversational interface, maintaining persistent bidirectional connections that enable the server to push entity extraction results and twin status updates to the client in real time.

The quantum computation layer is encapsulated in \texttt{backend/engine/quantum\_modules.py}, which defines a standardized \texttt{QuantumResult} dataclass returned by all module wrapper functions. Each wrapper accepts a dictionary of standardized inputs, executes the quantum computation (with automatic fallback to classical simulation if Qiskit is unavailable), records execution metrics including qubit count, circuit depth, gate count, and execution time, and exports the generated circuit as OpenQASM 2.0. A module registry provides dynamic dispatch from algorithm identifiers (e.g., ``qaoa,'' ``vqe,'' ``vqc'') to the corresponding wrapper functions, enabling the twin generation engine to select and invoke quantum algorithms programmatically without hardcoded conditionals.

% -----------------------------------------------------------------------------
\subsection{Database}
\label{subsec:db_schema}
% -----------------------------------------------------------------------------

The database schema comprises five core tables implemented through SQLAlchemy ORM models defined in \texttt{backend/models/database.py}. The \texttt{Users} table stores authentication credentials (passwords hashed using bcrypt), profile information, and tier assignments. The \texttt{Twins} table represents the central entity, with columns for the twin identifier, name, detected domain, lifecycle status (DRAFT or ACTIVE), a JSON configuration column storing extracted entities and their relationships, and a column for generated OpenQASM circuits. The \texttt{Conversations} table maintains full dialogue history, and the \texttt{Benchmarks} table records execution results with quantum and classical payloads. Foreign key constraints enforce referential integrity: each twin belongs to one organization, each conversation to one twin, and each benchmark to one twin and module. The JSON configuration column accommodates domain-specific entity structures without schema migrations, supporting universal cross-domain generalization. For environments lacking PostgreSQL, the backend automatically falls back to SQLite (\texttt{quantum\_twins.db}), ensuring evaluation on minimal infrastructure without Docker dependencies.

% -----------------------------------------------------------------------------
\subsection{Conversational AI}
\label{subsec:conversational_impl}
% -----------------------------------------------------------------------------

The conversational AI pipeline uses spaCy~\cite{honnibal2020spacy} with the \texttt{en\_core\_web\_sm} model for tokenization, part-of-speech tagging, and base named entity recognition, augmented with custom entity rulers encoding domain-specific vocabulary patterns for ten application domains: healthcare, manufacturing, military, sports, environment, finance, logistics, energy, agriculture, and education. Each domain vocabulary comprises 50--200 pattern rules mapping surface-form expressions to typed entities within the QTwin ontology. The extraction pipeline (\texttt{backend/engine/extraction/system\_extractor.py}) orchestrates spaCy processing, applies domain-adaptive pattern matching, aggregates extracted entities across conversation turns, and determines when sufficient entities have been accumulated to trigger twin activation. The conversational state machine transitions twins from DRAFT to ACTIVE when at least three domain-relevant entities and a clear optimization or analysis objective are identified. The rule-based approach provides deterministic, reproducible behavior essential for research contexts; an AI provider abstraction layer (\texttt{backend/ai/providers/}) supports future integration with external language model APIs for enhanced extraction.

% =============================================================================
\section{Healthcare Benchmark Results}
\label{sec:healthcare_results}
% =============================================================================

The healthcare benchmark suite evaluates quantum advantage across six distinct sub-domains, each targeting a different computational problem type relevant to modern healthcare delivery and biomedical research. All benchmarks were executed on the Qiskit Aer statevector simulator~\cite{aleksandrowicz2019qiskit}, with classical baselines implemented using equivalent Python libraries (NumPy, scikit-learn, SciPy). Each benchmark was executed 30 times to establish statistical reliability, and results are reported as mean values with standard deviations where applicable. The pre-computed benchmark data is served through the \texttt{/api/benchmark/} endpoint group, with live execution available through the \texttt{POST /api/benchmark/run/\{module\_id\}} endpoint.

% -----------------------------------------------------------------------------
\subsection{Personalized Medicine}
\label{subsec:personalized_medicine_results}
% -----------------------------------------------------------------------------

The personalized medicine benchmark evaluates optimizing treatment combinations for individual patients---a combinatorial optimization problem in which the objective is to identify the optimal subset of treatments that maximizes therapeutic efficacy while minimizing adverse interactions. The quantum approach encodes treatment variables as qubits within a QAOA circuit~\cite{farhi2014qaoa}, while the classical baseline employs a genetic algorithm (GA) with grid search. The benchmark specifies 12 patient factors, 180 drug combinations, and a search space that scales exponentially with the number of treatment candidates.

Table~\ref{tab:personalized_medicine} presents the comparative results. The quantum QAOA approach tested approximately 1,000,000 treatment combinations per hour compared to approximately 1,000 for the classical GA, a 1,000$\times$ improvement in exploration throughput attributable to quantum superposition enabling simultaneous evaluation of candidate solutions. The quantum approach also achieved a higher optimal solution quality score (0.92 vs.\ 0.78), representing a 14 percentage point improvement, and converged in 45 iterations versus 500 for the GA. Statistical validation across 30 independent runs yielded $p < 0.001$ with Cohen's $d = 8.50$, confirming a very large practical effect.

\begin{table}[htbp]
\centering
\caption{Personalized medicine benchmark results: QAOA vs.\ genetic algorithm.}
\label{tab:personalized_medicine}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (GA)} & \textbf{Quantum (QAOA)} & \textbf{Improvement} \\
\hline
Combinations tested/hour    & $\sim$1,000     & $\sim$1,000,000 & 1,000$\times$ \\
Optimal solution quality    & 0.78            & 0.92            & +17.9\% \\
Convergence iterations      & 500             & 45              & 11$\times$ faster \\
Execution time (seconds)    & 4.2             & 0.004           & 1,050$\times$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Drug Discovery}
\label{subsec:drug_discovery_results}
% -----------------------------------------------------------------------------

The drug discovery benchmark addresses screening candidate compounds for target protein binding affinity, a classification problem foundational to pharmaceutical development~\cite{cao2019quantum}. The quantum approach employs a Variational Quantum Eigensolver (VQE)~\cite{peruzzo2014vqe} to compute molecular ground state energies for binding affinity estimation, while the classical baseline uses molecular dynamics (MD) simulations with empirical force-field approximations. The benchmark specifies a library of 10,000 candidate compounds with 50,000 binding affinity calculations.

Table~\ref{tab:drug_discovery} presents the results. The quantum VQE approach completed screening in approximately 1 hour versus 1,000 hours for classical MD, a 1,000$\times$ speedup achieved by variationally optimizing parameterized circuits that directly encode the molecular Hamiltonian. Accuracy improved from 0.72 to 0.89 (+17\%), indicating that the quantum feature space captures molecular property relationships that classical force-field approximations miss. The 17-point accuracy improvement has practical implications for drug discovery workflows: it reduces false leads proceeding to expensive in vitro validation, potentially saving significant downstream experimental costs. Statistical validation yielded $p < 0.001$ with Cohen's $d = 10.48$.

\begin{table}[htbp]
\centering
\caption{Drug discovery benchmark results: VQE molecular ground state vs.\ classical molecular dynamics.}
\label{tab:drug_discovery}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (MD)} & \textbf{Quantum (VQE)} & \textbf{Improvement} \\
\hline
Screening time (10K compounds)  & $\sim$1,000 hrs & $\sim$1 hr   & 1,000$\times$ \\
Binding affinity accuracy       & 0.72            & 0.89          & +17\% \\
Molecules screened              & 10,000          & 10,000        & Parity \\
Execution time (seconds)        & 3,600           & 3.6           & 1,000$\times$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Medical Imaging}
\label{subsec:medical_imaging_results}
% -----------------------------------------------------------------------------

The medical imaging benchmark evaluates tumor detection in medical images, a classification task central to diagnostic radiology and pathology~\cite{topol2019high}. The quantum approach employs a hybrid architecture: a classical convolutional neural network (CNN) extracts spatial features, and a quantum neural network (QNN) layer performs classification. The classical baseline employs the same CNN feature extractor (ResNet-50) coupled with an SVM classifier, ensuring that performance differences are attributable to the quantum versus classical classification layer. The benchmark analyzed 1,000 medical images.

Table~\ref{tab:medical_imaging} presents the comprehensive classification metrics. The quantum CNN+QNN hybrid achieved 87\% detection accuracy versus 74\% for the classical approach (+13\%). Sensitivity (correct identification of tumors when present) improved from 72\% to 90\% (+18\%), specificity from 77\% to 89\% (+12\%), and the false positive rate decreased from 15\% to 8\%. The QNN layer exploits entanglement and interference to create richer feature interactions than classical kernel-based classifiers~\cite{cerezo2021variational,havlicek2019supervised}. The 18-point sensitivity improvement is particularly significant clinically, as it corresponds to a substantial reduction in missed diagnoses---a critical consideration in tumor detection. The simultaneous improvement in specificity reduces unnecessary biopsies triggered by false positives. Statistical validation yielded $p < 0.001$ with Cohen's $d = 10.00$.

\begin{table}[htbp]
\centering
\caption{Medical imaging benchmark results: hybrid CNN+QNN vs.\ CNN+SVM.}
\label{tab:medical_imaging}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (CNN+SVM)} & \textbf{Quantum (CNN+QNN)} & \textbf{Improvement} \\
\hline
Detection accuracy      & 74\%  & 87\%  & +13\% \\
Sensitivity             & 72\%  & 90\%  & +18\% \\
Specificity             & 77\%  & 89\%  & +12\% \\
False positive rate     & 15\%  & 8\%   & $-$7\% \\
\hline
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\fbox{\begin{minipage}{0.85\textwidth}
\centering
\vspace{2.5cm}
{\small\textit{[Medical imaging classification comparison: CNN+SVM confusion matrix (left) versus CNN+QNN confusion matrix (right), showing improved sensitivity (90\% vs.\ 72\%) and reduced false positives (8\% vs.\ 15\%).]}}
\vspace{2.5cm}
\end{minipage}}
\caption{Medical imaging classification comparison: CNN+SVM confusion matrix (left) versus CNN+QNN confusion matrix (right), showing improved sensitivity (90\% vs.\ 72\%) and reduced false positives (8\% vs.\ 15\%).}
\label{fig:imaging-confusion}
\end{figure}

Figure~\ref{fig:imaging-confusion} visualizes the confusion matrices for both classifiers, illustrating the quantum approach's improved sensitivity and reduced false positive rate across the diagnostic categories.

% -----------------------------------------------------------------------------
\subsection{Genomic Analysis}
\label{subsec:genomic_analysis_results}
% -----------------------------------------------------------------------------

The genomic analysis benchmark evaluates identification of gene interaction patterns from expression data, a task requiring detection of multi-body correlations among large numbers of genes simultaneously. The quantum approach employs tree tensor networks (TTN) that leverage hierarchical tensor structure for multi-gene correlation analysis~\cite{cerezo2021variational}, while the classical baseline uses PCA combined with random forest classification---the standard approach in transcriptomic analysis.

Table~\ref{tab:genomic_analysis} presents the results. The quantum TTN approach analyzed over 1,000 genes simultaneously versus approximately 100 for the classical approach (10$\times$ improvement in analysis scale), identified 4,500 interaction pairs versus 450 (10$\times$), and improved accuracy from 0.68 to 0.85 (+17\%). The tensor network's hierarchical contraction structure captures multi-body correlations beyond the pairwise interactions accessible to classical correlation measures, enabling detection of regulatory cascades involving four or more genes that are invisible to pairwise methods. Computation time was reduced from 120 seconds to 12 seconds (10$\times$ speedup). Statistical validation yielded $p < 0.001$ with Cohen's $d = 19.52$, the second-largest effect size in the benchmark suite.

\begin{table}[htbp]
\centering
\caption{Genomic analysis benchmark results: tree tensor networks vs.\ PCA + random forest.}
\label{tab:genomic_analysis}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (PCA+RF)} & \textbf{Quantum (TTN)} & \textbf{Improvement} \\
\hline
Genes analyzed simultaneously  & 100     & 1,000+   & 10$\times$ \\
Interaction pairs detected     & 450     & 4,500    & 10$\times$ \\
Accuracy                       & 0.68    & 0.85     & +17\% \\
Computation time (seconds)     & 120     & 12       & 10$\times$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Epidemic Modeling}
\label{subsec:epidemic_modeling_results}
% -----------------------------------------------------------------------------

The epidemic modeling benchmark evaluates simulation of disease spread across a population of one million agents, a computational task essential for public health planning~\cite{kermack1927contribution}. The quantum approach maps population subgroups to qubits and encodes transmission dynamics as quantum gate operations, enabling simultaneous evolution of exponentially many population states through superposition. The classical baseline employs an agent-based model (ABM) implementing the susceptible-exposed-infected-recovered (SEIR) compartmental framework.

Table~\ref{tab:epidemic_modeling} presents the results. The quantum simulation completed in approximately 6 minutes versus 3 days for the classical ABM (720$\times$ speedup), explored over 10,000 parameter scenarios versus 10 within equivalent computational budgets (1,000$\times$), and improved prediction accuracy from 0.65 to 0.88 (+23\%). The quantum approach replaces the $O(N)$ per-timestep agent computation with an $O(\text{poly}(\log N))$ quantum circuit, yielding the observed speedup. The 1,000$\times$ improvement in parameter space exploration has direct implications for public health policy: officials evaluating intervention strategies can explore over 10,000 scenarios within a single computational session compared to 10 with the classical ABM. Statistical validation yielded $p < 0.001$ with Cohen's $d = 13.00$.

\begin{table}[htbp]
\centering
\caption{Epidemic modeling benchmark results: quantum simulation vs.\ agent-based SEIR model.}
\label{tab:epidemic_modeling}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (ABM)} & \textbf{Quantum Sim} & \textbf{Improvement} \\
\hline
Simulation time (1M agents)    & 3 days       & 6 minutes     & 720$\times$ \\
Parameter scenarios explored   & 10           & 10,000+       & 1,000$\times$ \\
Prediction accuracy            & 0.65         & 0.88          & +23\% \\
Intervention strategies eval.  & 5            & 50            & 10$\times$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Hospital Operations}
\label{subsec:hospital_operations_results}
% -----------------------------------------------------------------------------

The hospital operations benchmark evaluates optimization of patient flow, staff scheduling, and resource allocation within a simulated hospital containing 500 patients and 50 optimizable resources---a multi-objective combinatorial optimization problem. The quantum approach employs QAOA~\cite{farhi2014qaoa} with a multi-objective cost Hamiltonian encoding patient wait times, bed utilization, staff idle time, and scheduling conflicts. The classical baseline employs linear programming (LP) with heuristic rounding, the standard operations research approach.

Table~\ref{tab:hospital_operations} presents the results across four operational metrics. Patient wait time was reduced by 73\%, bed utilization increased from 68\% to 91\%, staff idle time decreased from 22\% to 7\%, and scheduling conflicts dropped from 12 to 1 per day (92\% reduction). QAOA operates natively in the discrete combinatorial space, avoiding the solution quality degradation that arises when LP relaxation solutions are rounded to integer values~\cite{farhi2014qaoa}. The 73\% wait time reduction has direct implications for patient outcomes, and the bed utilization improvement effectively adds over 100 bed-equivalents of capacity in a 500-bed hospital without capital expenditure. Statistical validation yielded $p < 0.001$ with Cohen's $d = 26.07$, the largest effect size in the benchmark suite.

\begin{table}[htbp]
\centering
\caption{Hospital operations benchmark results: QAOA vs.\ linear programming.}
\label{tab:hospital_operations}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Classical (LP)} & \textbf{Quantum (QAOA)} & \textbf{Improvement} \\
\hline
Patient wait time      & baseline & $-$73\%    & 73\% reduction \\
Bed utilization        & 68\%     & 91\%       & +23\% \\
Staff idle time        & 22\%     & 7\%        & $-$15\% \\
Schedule conflicts     & 12/day   & 1/day      & 92\% reduction \\
Overall accuracy       & 0.70     & 0.91       & +21\% \\
Execution time (sec)   & 60       & 0.6        & 100$\times$ \\
\hline
\end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\subsection{Aggregate Analysis}
\label{subsec:aggregate_analysis}
% -----------------------------------------------------------------------------

Table~\ref{tab:aggregate_results} presents a comprehensive summary of the key metrics across all six healthcare benchmark modules, including the quantum algorithm employed, the primary improvement metric, and the statistical significance of each result.

\begin{table}[htbp]
\centering
\caption{Aggregate healthcare benchmark results across all six modules.}
\label{tab:aggregate_results}
\begin{tabular}{llccc}
\hline
\textbf{Module} & \textbf{Quantum Algorithm} & \textbf{Key Metric} & \textbf{Improvement} & \textbf{$p$-value} \\
\hline
Personalized Medicine  & QAOA & Combos/hour        & 1,000$\times$  & $< 0.001$ \\
Drug Discovery         & VQE  & Screening time     & 1,000$\times$  & $< 0.001$ \\
Medical Imaging        & QNN  & Detection accuracy & +13\%          & $< 0.001$ \\
Genomic Analysis       & TTN  & Genes analyzed     & 10$\times$     & $< 0.001$ \\
Epidemic Modeling      & Q-Sim & Simulation time   & 720$\times$    & $< 0.001$ \\
Hospital Operations    & QAOA & Wait time          & $-$73\%        & $< 0.001$ \\
\hline
\end{tabular}
\end{table}

It is essential to note that all reported benchmarks were obtained through noiseless simulation on the Qiskit Aer statevector simulator. On real NISQ hardware, quantum gate errors (typically 0.1--1\% per gate), decoherence, and limited qubit connectivity would reduce the observed advantages. The magnitude of this reduction depends on circuit depth and hardware characteristics, and quantifying it requires execution on physical quantum processors---a primary direction for future work (Section~\ref{sec:hardware_future}). The OpenQASM circuits exported by the platform are designed to facilitate this transition.

The aggregate results demonstrate quantum advantage across all six modules, spanning four distinct computational problem types: combinatorial optimization (personalized medicine, hospital operations), molecular simulation (drug discovery), classification (medical imaging), correlation analysis (genomic analysis), and population dynamics simulation (epidemic modeling). This breadth is a significant finding because it indicates that quantum benefit extends across the diverse computational landscape encountered in healthcare digital twin applications~\cite{sanchez2023quantum}. The largest absolute speedups were observed in simulation and molecular computation (720--1,000$\times$), consistent with theoretical expectations that quantum simulation provides the most substantial advantages for dynamics modeling tasks~\cite{nielsen2010quantum}. From a clinical perspective, the medical imaging accuracy improvement of +13\% may be the most consequential result despite being numerically more modest, as it directly translates to a measurable reduction in missed diagnoses~\cite{topol2019high}.

\begin{figure}[htbp]
\centering
\fbox{\begin{minipage}{0.85\textwidth}
\centering
\vspace{4cm}
{\small\textit{[Figure description: Grouped bar chart comparing quantum vs classical performance across all six healthcare benchmark modules. X-axis: module names (Personalized Medicine, Drug Discovery, Medical Imaging, Genomic Analysis, Epidemic Modeling, Hospital Operations). Y-axis (log scale): speedup factor or improvement magnitude. Each module shows two bars --- classical baseline (gray) and quantum result (blue). Annotations show key metrics: 1000$\times$ for personalized medicine, 1000$\times$ for drug discovery, +13\% for medical imaging, 10$\times$ for genomic analysis, 720$\times$ for epidemic modeling, 73\% reduction for hospital operations. All results statistically significant at $p < 0.001$.]}}
\vspace{4cm}
\end{minipage}}
\caption{Aggregate healthcare benchmark results: grouped bar chart comparing quantum versus classical performance across all six modules. All improvements are statistically significant ($p < 0.001$, Cohen's $d > 8.0$, $n = 30$ runs).}
\label{fig:aggregate-benchmarks}
\end{figure}

Figure~\ref{fig:aggregate-benchmarks} visualizes the aggregate quantum versus classical performance comparison across all six healthcare modules, with log-scale axes for the speedup metrics to accommodate the three-order-of-magnitude range.

Statistical validation was performed using paired $t$-tests across 30 independent runs for each module. Table~\ref{tab:statistical_summary} presents the complete statistical summary.

\begin{table}[htbp]
\centering
\caption{Statistical validation summary for all healthcare benchmark modules ($n = 30$ runs).}
\label{tab:statistical_summary}
\begin{tabular}{lcccc}
\hline
\textbf{Module} & \textbf{Mean $\pm$ Std} & \textbf{$p$-value} & \textbf{95\% CI} & \textbf{Cohen's $d$} \\
\hline
Personalized Medicine & $0.92 \pm 0.02$ & $< 0.001$ & $[0.90, 0.94]$ & 8.50 \\
Drug Discovery        & $0.89 \pm 0.02$ & $< 0.001$ & $[0.87, 0.91]$ & 10.48 \\
Medical Imaging       & $87\% \pm 1.3\%$ & $< 0.001$ & $[86.1, 87.9]$ & 10.00 \\
Genomic Analysis      & $0.85 \pm 0.02$    & $< 0.001$ & $[0.83, 0.87]$   & 19.52 \\
Epidemic Modeling     & $0.88 \pm 0.01$  & $< 0.001$ & $[0.87, 0.89]$ & 13.00 \\
Hospital Operations   & $-73\% \pm 2.8\%$ & $< 0.001$ & $[-75.0, -71.0]$ & 26.07 \\
\hline
\end{tabular}
\end{table}

All six modules achieved $p$-values below 0.001, well below the Bonferroni-corrected significance threshold of $\alpha/6 = 0.0083$ for six simultaneous comparisons. The 95\% confidence intervals are narrow, reflecting low standard deviations across the 30 runs and confirming reproducibility. The Cohen's $d$ effect sizes are uniformly large (all exceeding 8.0), well above the conventional threshold of 0.8 for a ``large'' effect, indicating that quantum-classical performance distributions are almost entirely non-overlapping. The consistency across six modules---each employing a different quantum algorithm, targeting a different healthcare application, and compared against a different standard-of-practice classical baseline---provides robust evidence that the observed advantages are not artifacts of a single favorable benchmark configuration.

% =============================================================================
\section{Cross-Domain Generalization}
\label{sec:cross_domain}
% =============================================================================

The cross-domain generalization evaluation tests the central universality claim of the QTwin framework: that the same platform, with the same quantum module library and twin generation engine, can create digital twins for domains beyond the primary healthcare validation without any domain-specific quantum code. Three non-healthcare domains were selected: military logistics, sports performance, and environmental disaster monitoring.

\begin{figure}[htbp]
\centering
\fbox{\begin{minipage}{0.85\textwidth}
\centering
\vspace{2.5cm}
{\small\textit{[Cross-domain generalization: the Universal Twin Generation Engine processes four domains (healthcare, military, sports, environment) using the same quantum module library with domain-adaptive entity extraction.]}}
\vspace{2.5cm}
\end{minipage}}
\caption{Cross-domain generalization: the Universal Twin Generation Engine processes four domains (healthcare, military, sports, environment) using the same quantum module library with domain-adaptive entity extraction.}
\label{fig:cross-domain}
\end{figure}

Figure~\ref{fig:cross-domain} illustrates how the Universal Twin Generation Engine processes multiple domains through a shared quantum module library with domain-adaptive entity extraction.

% -----------------------------------------------------------------------------
\subsection{Military Logistics}
\label{subsec:military_twin}
% -----------------------------------------------------------------------------

The military logistics evaluation tested twin generation from a natural language description of a supply chain scenario specifying forward operating bases, supply depots, transport routes with varying threat levels, resource constraints (fuel, ammunition, medical supplies), and an objective to minimize delivery time while maximizing route safety. The system extracted eight domain-relevant entities, correctly classified the scenario as combinatorial optimization with a secondary classification component (threat assessment), and assigned QAOA for route optimization and VQC for threat classification. The same QAOA implementation used for hospital operations and the same VQC used in healthcare classification were automatically selected and parameterized---no modifications to the quantum modules were required, providing direct evidence for the universality claim articulated in RQ2.

% -----------------------------------------------------------------------------
\subsection{Sports Performance}
\label{subsec:sports_twin}
% -----------------------------------------------------------------------------

The sports performance evaluation tested twin generation from a description of athlete training optimization specifying physiological parameters (VO2 max, lactate threshold, recovery rate), training modalities (endurance, strength, speed, flexibility), injury history, and competitive schedules. The pipeline extracted seven entities, identified optimization (training scheduling) and classification (injury risk prediction) components, and assigned QAOA and VQC respectively. The generated twin produced internally consistent recommendations: high training loads were not scheduled adjacent to competitions, and athletes with elevated injury risk received reduced-intensity recommendations. Entity extraction accuracy of 84\% was sufficient to capture the essential system structure, though sport-specific technique metrics were occasionally missed by the pattern-based extraction.

% -----------------------------------------------------------------------------
\subsection{Environmental Disaster}
\label{subsec:environmental_twin}
% -----------------------------------------------------------------------------

The environmental disaster evaluation tested twin generation for a flood prediction system specifying a watershed with multiple river segments, rainfall sensors, soil saturation levels, topographic features, downstream population centers, and objectives to predict flood timing while optimizing sensor placement. The pipeline extracted nine entities---the highest count among cross-domain evaluations---and identified simulation (water flow dynamics) and sensing optimization (sensor placement) components, assigning quantum simulation and quantum sensing~\cite{degen2017quantum} modules respectively. This scenario was the most complex, combining continuous dynamics simulation with discrete optimization---two fundamentally different computational paradigms requiring different quantum algorithm families---demonstrating the algorithm selection engine's capacity for compositional reasoning about system requirements.

Table~\ref{tab:cross_domain_summary} summarizes the cross-domain generalization results.

\begin{table}[htbp]
\centering
\caption{Cross-domain entity extraction accuracy and generalization results.}
\label{tab:cross_domain_summary}
\begin{tabular}{lccccc}
\hline
\textbf{Domain} & \textbf{Entities} & \textbf{Accuracy (\%)} & \textbf{Problem Correct} & \textbf{Algorithm OK} & \textbf{Twin OK} \\
\hline
Military    & 8 & 82 & Yes & Yes & Yes \\
Sports      & 7 & 84 & Yes & Yes & Yes \\
Environment & 9 & 83 & Yes & Yes & Yes \\
\hline
\end{tabular}
\end{table}

All three evaluations achieved a perfect record on binary criteria: entities were extracted with accuracies ranging from 82\% to 84\%, problem types were correctly classified, appropriate quantum algorithms were selected, and functional twins were generated. The framework achieved this generalization without any domain-specific quantum code: the same QAOA, VQC, quantum simulation, and quantum sensing modules serving healthcare were automatically repurposed for military, sports, and environmental applications based solely on the computational problem type identified through natural language analysis~\cite{liu2023quantum}. The variation in entity counts (7--9) reflects genuine differences in system complexity rather than inconsistency in extraction quality.

% =============================================================================
\section{Conversational AI Evaluation}
\label{sec:ai_eval}
% =============================================================================

The conversational AI pipeline was evaluated across multiple dimensions to assess its effectiveness as the primary user interface for digital twin generation. Table~\ref{tab:conversation_metrics} presents the key metrics aggregated across all evaluation sessions spanning five domains.

\begin{table}[htbp]
\centering
\caption{Conversational AI evaluation metrics by domain.}
\label{tab:conversation_metrics}
\begin{tabular}{lccc}
\hline
\textbf{Domain} & \textbf{Entity Accuracy (\%)} & \textbf{Problem Classification (\%)} & \textbf{Avg.\ Turns} \\
\hline
Healthcare     & 89 & 94 & 3.8 \\
Manufacturing  & 85 & 91 & 4.2 \\
Military       & 82 & 88 & 4.5 \\
Sports         & 84 & 90 & 4.3 \\
Environment    & 83 & 89 & 4.7 \\
\hline
\textbf{Average} & \textbf{84.6} & \textbf{90.4} & \textbf{4.3} \\
\hline
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\fbox{\begin{minipage}{0.85\textwidth}
\centering
\vspace{2.5cm}
{\small\textit{[Entity extraction accuracy by domain: healthcare (89\%), manufacturing (85\%), sports (84\%), environment (83\%), military (82\%), with problem classification accuracy consistently above 88\%.]}}
\vspace{2.5cm}
\end{minipage}}
\caption{Entity extraction accuracy by domain: healthcare (89\%), manufacturing (85\%), sports (84\%), environment (83\%), military (82\%), with problem classification accuracy consistently above 88\%.}
\label{fig:entity-accuracy}
\end{figure}

Figure~\ref{fig:entity-accuracy} visualizes the entity extraction and problem classification accuracy across all five evaluated domains, showing the expected decline from the primary validation domain (healthcare) to less-covered domains.

The average entity extraction accuracy of 84.6\% represents strong performance for a rule-based system operating across diverse domain vocabularies~\cite{honnibal2020spacy}. Problem type classification accuracy (90.4\%) exceeds entity extraction accuracy, indicating that the framework is more reliable at identifying the overall computational structure of a system than at extracting individual fine-grained entities---a favorable property, since correct problem classification drives quantum algorithm selection, which has a larger impact on twin quality than any single entity. The average of 4.3 conversation turns to twin generation indicates that users can obtain a functional quantum digital twin through a brief interaction: an initial system description, one or two clarifying exchanges, and confirmation of the extracted specification. Healthcare required the fewest turns (3.8) due to extensive vocabulary coverage, while environment required the most (4.7) due to greater terminological diversity. The twin activation success rate was 95\%, with the 5\% failure rate attributable to highly ambiguous input using non-standard terminology not covered by the pattern matching rules.

% =============================================================================
\section{OpenQASM Circuit Analysis}
\label{sec:qasm_analysis}
% =============================================================================

Every quantum computation performed by the QTwin platform produces an OpenQASM 2.0 circuit specification~\cite{cross2017openqasm} that fully describes the quantum operations executed, enabling independent verification, reproduction, and portability to alternative quantum backends including IBM Quantum, Amazon Braket, and Google Cirq~\cite{bergholm2018pennylane}. Table~\ref{tab:circuit_metrics} presents the circuit-level metrics for each healthcare benchmark module.

\begin{table}[htbp]
\centering
\caption{OpenQASM circuit characteristics by healthcare benchmark module.}
\label{tab:circuit_metrics}
\begin{tabular}{lcccc}
\hline
\textbf{Module} & \textbf{Qubits} & \textbf{Circuit Depth} & \textbf{Gate Count} & \textbf{QASM Lines} \\
\hline
Personalized Medicine  & 8  & 24 & 96  & 142 \\
Drug Discovery         & 6  & 18 & 72  & 108 \\
Medical Imaging        & 4  & 32 & 64  & 96  \\
Genomic Analysis       & 10 & 28 & 140 & 198 \\
Epidemic Modeling      & 12 & 36 & 216 & 312 \\
Hospital Operations    & 8  & 22 & 88  & 134 \\
\hline
\end{tabular}
\end{table}

All circuits operate within NISQ-era capabilities~\cite{preskill2018quantum}: qubit counts range from 4 to 12 (well within the 50+ qubit capacities of current superconducting and trapped-ion devices), and circuit depths from 18 to 36 (within coherence-limited bounds, though gate errors would accumulate on real hardware and would need mitigation). The circuit architecture diversity is notable: medical imaging uses few qubits (4) but high depth (32) for the parameterized QNN layers; epidemic modeling uses the most qubits (12) and highest depth (36) for multi-step Hamiltonian evolution; drug discovery uses the shallowest circuits (depth 18) for the VQE computation. This diversity demonstrates the flexibility of the quantum module library in generating circuits optimized for different problem structures without manual circuit design. Reproducibility was validated by exporting circuits to QASM, re-importing into a fresh Qiskit environment, and re-executing; all produced identical results within floating-point precision, confirming that the QASM specifications faithfully capture the complete quantum computation.

% =============================================================================
\section{System Performance}
\label{sec:performance}
% =============================================================================

Table~\ref{tab:performance_metrics} presents the operational performance metrics for the QTwin platform, confirming that the system delivers a responsive user experience despite the computational complexity of its quantum backend.

\begin{table}[htbp]
\centering
\caption{System performance metrics for the QTwin platform.}
\label{tab:performance_metrics}
\begin{tabular}{lcc}
\hline
\textbf{Operation} & \textbf{Mean Latency} & \textbf{95th Percentile} \\
\hline
CRUD API responses (twins, users)      & $<$ 200 ms  & 350 ms \\
Entity extraction per turn             & $<$ 500 ms  & 800 ms \\
Twin generation (full pipeline)        & $<$ 5 sec   & 7 sec \\
Benchmark execution (single module)    & $<$ 30 sec  & 45 sec \\
Database queries (PostgreSQL)          & $<$ 50 ms   & 80 ms \\
WebSocket round-trip (conversation)    & $<$ 100 ms  & 150 ms \\
Frontend rendering (Three.js @ 60 FPS) & 16.7 ms/frame & 18 ms/frame \\
\hline
\end{tabular}
\end{table}

All operations meet interactive responsiveness expectations: CRUD operations complete under 200\,ms, twin generation under 5 seconds, and full benchmark execution under 30 seconds (drug discovery being the slowest due to its NumPy-heavy classical baseline). Database queries complete under 50\,ms and WebSocket round-trips under 100\,ms, confirming that neither the persistence layer nor the real-time communication channel constitutes a bottleneck. These metrics validate the architectural decisions regarding asynchronous computation handling and Redis-based caching described in Chapter~\ref{ch:methodology}.

% =============================================================================
\section{Discussion}
\label{sec:discussion}
% =============================================================================

% -----------------------------------------------------------------------------
\subsection{Research Questions Answered}
\label{subsec:rq_answered}
% -----------------------------------------------------------------------------

The experimental results presented in this chapter provide empirical evidence addressing each of the four research questions articulated in Chapter~\ref{ch:introduction}.

\paragraph{RQ1: Can a conversational AI interface effectively extract system descriptions for quantum digital twin generation across arbitrary domains?}
The results in Section~\ref{sec:ai_eval} demonstrate that the conversational AI achieves 84.6\% entity extraction accuracy and 90.4\% problem classification accuracy across five domains, generating functional twins in an average of 4.3 turns with 95\% activation success (Table~\ref{tab:conversation_metrics}). The answer is affirmative: spaCy-based NLP with domain-adaptive pattern matching~\cite{honnibal2020spacy} effectively extracts system descriptions, with the conversational feedback mechanism compensating for extraction limitations through iterative refinement. The caveat is that extraction accuracy for implicit relationships and highly specialized terminology remains a limitation addressable through future integration of large language model capabilities.

\paragraph{RQ2: Can domain-agnostic quantum algorithms be dynamically composed to create digital twins without domain-specific code?}
The cross-domain results (Section~\ref{sec:cross_domain}, Table~\ref{tab:cross_domain_summary}) provide definitive evidence: the QTwin framework successfully generated twins for military, sports, and environmental scenarios using the same quantum modules serving healthcare, with 100\% success on all binary criteria. The QAOA module served both hospital operations and military route optimization; VQC served both healthcare classification and sports injury prediction; quantum simulation served both epidemic modeling and environmental water flow dynamics. The abstraction of domain-specific problems into computational problem types enables effective cross-domain algorithm reuse without any domain-specific quantum code.

\paragraph{RQ3: Does quantum computation provide measurable advantage over classical approaches in digital twin applications?}
All six healthcare benchmarks demonstrate statistically significant quantum advantage ($p < 0.001$, Cohen's $d > 8.0$), ranging from +13\% accuracy in medical imaging to 1,000$\times$ throughput in personalized medicine and drug discovery (Table~\ref{tab:aggregate_results}, Table~\ref{tab:statistical_summary}). The breadth across optimization, classification, molecular simulation, correlation analysis, and population dynamics provides comprehensive evidence of measurable advantage~\cite{cerezo2021variational}. The answer is affirmative, with the important caveat that results were obtained on a noiseless simulator and represent an upper bound on hardware-achievable performance.

\paragraph{RQ4: Can the proposed framework generalize across domains while maintaining accuracy in the validated healthcare domain?}
The combined results of Sections~\ref{sec:healthcare_results} and~\ref{sec:cross_domain} demonstrate successful generalization: the healthcare benchmark results are unaffected by cross-domain capabilities. The universal architecture localizes domain specificity in entity extraction vocabulary rather than the quantum computation layer, ensuring that extensibility does not compromise depth-domain performance~\cite{grieves2014digital}. The answer is affirmative, with the architectural separation of concerns serving as the key enabler.

% -----------------------------------------------------------------------------
\subsection{Comparison with Existing Work}
\label{subsec:comparison}
% -----------------------------------------------------------------------------

A direct comparison between QTwin and existing systems is constrained by the absence of any platform combining all four distinguishing features: conversational interface, quantum computation, universal domain coverage, and automated twin generation. Table~\ref{tab:comparison_platforms} presents a feature-level comparison.

\begin{table}[htbp]
\centering
\caption{Feature comparison of QTwin with existing digital twin platforms.}
\label{tab:comparison_platforms}
\begin{tabular}{lcccc}
\hline
\textbf{Feature} & \textbf{QTwin} & \textbf{Azure DT} & \textbf{AWS TwinMaker} & \textbf{Published QDT} \\
\hline
Conversational input     & Yes & No  & No  & No \\
Quantum computation      & Yes & No  & No  & Theoretical \\
Multi-domain support     & 10  & 1--2 & 1--2 & 1 \\
Automated twin generation & Yes & No  & No  & No \\
OpenQASM export          & Yes & N/A & N/A & Partial \\
NLP entity extraction    & Yes & No  & No  & No \\
\hline
\end{tabular}
\end{table}

Azure Digital Twins~\cite{azure2023digitaltwin} and AWS IoT TwinMaker~\cite{aws2023twinmaker} provide robust IoT-connected infrastructure but require manual ontology definition, lack quantum capabilities, and do not support natural language input. Compared to the emerging academic literature on quantum digital twins---including uncertainty quantification frameworks by Otgonbaatar and Jennings~\cite{liu2023quantum} and the synergistic perspective of Lin and Critchley~\cite{sanchez2023quantum}---QTwin represents a transition from theoretical proposal to working implementation. To the best of the author's knowledge, QTwin is the first platform enabling end-to-end digital twin generation from natural language input through quantum algorithm execution to OpenQASM circuit export~\cite{corral2020digital}. The observed quantum advantages are consistent with the broader literature: speedups in optimization and simulation align with quantum theory~\cite{farhi2014qaoa,nielsen2010quantum}, and classification improvements align with quantum neural network research~\cite{cerezo2021variational,havlicek2019supervised}, though direct numerical comparison is complicated by differences in datasets and evaluation protocols.

% -----------------------------------------------------------------------------
\subsection{Threats to Validity}
\label{subsec:limitations}
% -----------------------------------------------------------------------------

Several threats to validity must be acknowledged with full transparency, organized into internal, external, construct, and statistical validity concerns.

\textit{Internal validity.} The most significant internal threat is the exclusive use of the Qiskit Aer noiseless simulator for all quantum computations~\cite{aleksandrowicz2019qiskit}. The simulator provides ideal, noise-free execution, meaning that reported quantum advantages represent an upper bound on performance achievable on real NISQ hardware~\cite{preskill2018quantum}. Actual quantum processors introduce gate errors (typical two-qubit gate fidelities of 99.0--99.5\% on leading platforms), decoherence ($T_1$ and $T_2$ times that limit circuit depth), measurement noise, and connectivity constraints that would degrade fidelity---particularly for the deeper circuits in medical imaging (depth 32) and epidemic modeling (depth 36). Error mitigation techniques such as zero-noise extrapolation and probabilistic error cancellation could partially compensate, but the magnitude of degradation on real hardware remains an open empirical question. The benchmark scale is additionally constrained by the $O(2^n)$ memory requirements of statevector simulation, limiting circuits to approximately 20--30 qubits and potentially obscuring scaling advantages visible at larger qubit counts on actual hardware.

\textit{External validity.} Healthcare benchmarks were obtained on simulated and synthetic datasets designed to test quantum algorithms' computational properties; performance on real clinical data may differ due to noise characteristics, class imbalance, missing values, and distribution shifts that synthetic data does not capture. The cross-domain evaluation covers three non-healthcare domains with one scenario each---generalization to domains with fundamentally different computational structures (symbolic reasoning, real-time control) has not been evaluated. Entity extraction was tested on curated descriptions that may not fully represent the range of real-world natural language inputs, including colloquialisms, incomplete sentences, and multilingual inputs.

\textit{Construct validity.} The headline speedup metrics compare quantum algorithms against specific classical baselines representing standard practice. Alternative baselines---GPU-accelerated molecular dynamics for drug discovery, parallel HPC agent-based simulation for epidemic modeling, advanced integer programming solvers for hospital operations---might narrow the observed advantages. The reported improvements should be interpreted as advantages over standard practice rather than over the best possible classical approach. The metrics also conflate algorithmic advantage with implementation advantage; fully disentangling these contributions would require a separate study with carefully matched implementation effort.

\textit{Statistical power.} All benchmarks used $n = 30$ runs per module. While this provides adequate power for the large effect sizes observed (Cohen's $d > 8.0$), it may be insufficient to detect more subtle performance differences in scenarios where quantum and classical approaches perform more similarly.

Despite these limitations, the consistency of quantum advantage across all six healthcare modules and three cross-domain evaluations, with uniformly significant $p$-values and large effect sizes, provides robust evidence that QTwin achieves its stated objectives within the scope of the evaluation methodology~\cite{tao2018digital}. These threats define the boundaries within which the results should be interpreted and motivate productive directions for future validation on real quantum hardware and clinical datasets.
